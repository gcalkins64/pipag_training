{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51b044f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fbbdbe",
   "metadata": {},
   "source": [
    "TO RUN:\n",
    "- Make sure to select Runtime>Change Runtime Type>T4 GPU to use cuda\n",
    "- Install pytorch_lightning\n",
    "- clone in repo to read in common files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cf284",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97517fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from time import time\n",
    "import datetime\n",
    "import pickle\n",
    "import matplotlib.lines as mlines\n",
    "import sys\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86396875",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://gcalkins64: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@github.com/gcalkins64/pipag_training.git\n",
    "!cd pipag_training && git pull\n",
    "sys.path.append('/content/pipag_training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb167c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmvae_common import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180eae7",
   "metadata": {},
   "source": [
    "# Check Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set default tensor type to CUDA tensors\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246109d8",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0\n",
    "n_train = 1024\n",
    "n_val = 128\n",
    "n_test = 128\n",
    "# For testing small batches of data\n",
    "# n_train = 400\n",
    "# n_val = 50\n",
    "# n_test = 50\n",
    "hd1 = 64\n",
    "hd2 = 32\n",
    "hd3 = 16\n",
    "latent_dim = 4\n",
    "n_clusters = 5\n",
    "lr = 1e-3\n",
    "n_epochs = 30_000 # 30_000\n",
    "batch_size = 64\n",
    "em_reg = 1e-6\n",
    "decoder_var = 1e-5\n",
    "\n",
    "plot_interval = 1000\n",
    "dpi = 300\n",
    "\n",
    "K = n_clusters\n",
    "Z = latent_dim\n",
    "D = 1  #num_modalities\n",
    "\n",
    "downsampleNum = 64\n",
    "\n",
    "# Numpy random seed\n",
    "np.random.seed(r)\n",
    "\n",
    "# PyTorch random seed for CPU\n",
    "torch.manual_seed(r)\n",
    "\n",
    "# PyTorch random seed for all GPU operations (if using CUDA)\n",
    "torch.cuda.manual_seed(r)\n",
    "torch.cuda.manual_seed_all(r)\n",
    "\n",
    "# Ensure deterministic behavior for PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af3060",
   "metadata": {},
   "source": [
    "# Set Up Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestr = datetime.strftime(datetime.now(), \"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in old data\n",
    "# dirname = os.path.join(\"drive\", \"MyDrive\", \"GMVAE_guided_aerocapture\", 'gmvae_em_aerocapture_energy_20250213_215721')\n",
    "# Generate new data path\n",
    "dirname = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", \"gmvae_em_aerocapture_energy_\"+timestr)\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "print(\"Filepath directory: \" + dirname)\n",
    "\n",
    "postfix = '_{0:d}_{1:d}_{2:d}_{3:d}_{4:d}_{5:d}_{6:d}_{7:d}_{8:d}_{9:f}_{10:d}_{11:f}_{12:f}_{13:d}_'.format(\n",
    "          r, n_train, n_val, n_test, hd1, hd2, hd3, latent_dim, n_clusters, lr, batch_size, em_reg * 1e3, decoder_var, n_epochs)\n",
    "print(\"Filepath postfix: \" + postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a41155",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'UOP_training_data_5000_scaled_downsampled_energy'\n",
    "data_dir = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", f\"{data}.json\")\n",
    "# data_dir = f'{data}.pkl'\n",
    "\n",
    "data_module = AerocaptureDataModuleCUDA(data_dir=data_dir, n_train=n_train, n_val=n_val, n_test=n_test,\n",
    "                                  train_batch=batch_size, val_batch=batch_size, test_batch=batch_size,\n",
    "                                  num_workers=0)\n",
    "\n",
    "data_module.setup(\"fit\")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "data_dim = len(train_loader.dataset[0][0])\n",
    "text_labels = ['capture', 'escape']\n",
    "label_colors = ['C2', 'C3']\n",
    "\n",
    "num_train_batches = len(train_loader)\n",
    "\n",
    "ts_plot = np.linspace(0,450,64)\n",
    "seabornSettings()\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "for j in range(n_train):\n",
    "    # print(len(train_loader.dataset[j][0].cpu()))\n",
    "    ax.plot(ts_plot, train_loader.dataset[j][0].cpu(), color=label_colors[train_loader.dataset[j][1].cpu()], alpha=0.75)\n",
    "\n",
    "eline = mlines.Line2D([], [], color='C2', label='Escape')\n",
    "cline = mlines.Line2D([], [], color='C3', label='Capture')\n",
    "plt.legend(handles=[eline, cline])\n",
    "plt.hlines(0, 0, ts_plot[-1], colors='r', linestyles='dashed')\n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Nondimensionalized Energy\")\n",
    "plt.title(\"Training Data\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(dirname, 'train_data'+postfix+'.png'), dpi=dpi)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "for j in range(n_val):\n",
    "    ax.scatter(np.arange(downsampleNum), val_loader.dataset[j][0].cpu(), color=label_colors[val_loader.dataset[j][1].cpu()])\n",
    "plt.title(\"Validation Data\")\n",
    "fig.savefig(os.path.join(dirname, 'val_data'+postfix+'.png'), dpi=dpi)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "for j in range(n_test):\n",
    "    ax.scatter(np.arange(downsampleNum), test_loader.dataset[j][0].cpu(), color=label_colors[test_loader.dataset[j][1].cpu()])\n",
    "plt.title(\"Test Data\")\n",
    "fig.savefig(os.path.join(dirname, 'test_data'+postfix+'.png'), dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332f1ee",
   "metadata": {},
   "source": [
    "# Run GMVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize latent GMM model parameters\n",
    "params = {}\n",
    "pi_variables = torch.zeros(K).clone().detach().requires_grad_(True)\n",
    "params['pi_c'] = torch.ones(K) / K\n",
    "params['mu_c'] = torch.rand((K, Z)) * 2.0 - 1.0\n",
    "params['logsigmasq_c'] = torch.zeros((K, Z))\n",
    "\n",
    "\n",
    "text_labels = [f'Cluster {i}' for i in range((n_clusters))]\n",
    "label_colors = [f'C{i+1}' for i in range((n_clusters))]\n",
    "\n",
    "# initialize neural networks\n",
    "encoder_list = []\n",
    "decoder_list = []\n",
    "trainable_parameters = []\n",
    "trainable_parameters.append(pi_variables)\n",
    "\n",
    "for _ in range(D):\n",
    "    encoder = Encoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd1, hd2, hd3])\n",
    "    decoder = Decoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd3, hd2, hd1])\n",
    "    encoder_list.append(encoder)\n",
    "    decoder_list.append(decoder)\n",
    "    trainable_parameters += list(encoder.parameters()) + list(decoder.parameters())\n",
    "\n",
    "optimizer = optim.Adam(trainable_parameters, lr=lr)\n",
    "\n",
    "# training\n",
    "\n",
    "import time\n",
    "ts = time.time()\n",
    "tic = time.perf_counter()\n",
    "\n",
    "train_loss = torch.zeros(n_epochs)\n",
    "train_elbo_terms = torch.zeros((n_epochs, 4)) # 4 ELBO terms\n",
    "val_elbo_terms = torch.zeros((n_epochs, 4)) # 4 ELBO terms\n",
    "val_loss = torch.zeros(n_epochs)\n",
    "pi_history = torch.zeros((n_epochs, K))\n",
    "train_mse_history = torch.zeros(n_epochs)\n",
    "val_mse_history = torch.zeros(n_epochs)\n",
    "min_val_loss = torch.inf\n",
    "seabornSettings()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ti = time.time()\n",
    "    for encoder in encoder_list:\n",
    "        encoder.train()\n",
    "    for decoder in decoder_list:\n",
    "        decoder.train()\n",
    "\n",
    "    train_elbo = 0\n",
    "    train_mse = 0\n",
    "    train_elbo_term = np.zeros(4)\n",
    "    params['hist_weights'] = torch.zeros((K, 1))\n",
    "    params['hist_mu_c'] = torch.zeros((K, latent_dim))\n",
    "    params['hist_logsigmasq_c'] = torch.zeros((K, latent_dim))\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(train_loader):\n",
    "        batch_x, _ = batch\n",
    "        x_list = [batch_x]  # assume D=2 and each modality has data_dim\n",
    "        optimizer.zero_grad()\n",
    "        pi_c = torch.exp(pi_variables) / torch.sum(torch.exp(pi_variables))\n",
    "        params['pi_c'] = pi_c\n",
    "\n",
    "        mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
    "        sigma = torch.exp(0.5 * logsigmasq)\n",
    "        eps = Normal(0, 1).sample(mu.shape)\n",
    "        z = mu + eps * sigma\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gamma_c, mu_c, logsigmasq_c = em_step(z, mu, logsigmasq, params, em_reg, update_by_batch=True)\n",
    "        params['mu_c'] = mu_c\n",
    "        params['logsigmasq_c'] = logsigmasq_c\n",
    "\n",
    "        elbo, sse, elbo_terms = decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c)\n",
    "        train_elbo += elbo.item()\n",
    "        train_elbo_term += elbo_terms\n",
    "        train_mse += sse.item()\n",
    "        loss = - elbo / batch_x.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    for encoder in encoder_list:\n",
    "        encoder.eval()\n",
    "    for decoder in decoder_list:\n",
    "        decoder.eval()\n",
    "\n",
    "    if epoch % plot_interval == 0 or epoch == n_epochs:\n",
    "        # Plot the first two dimensions of the latents\n",
    "        with torch.no_grad():\n",
    "            means = []\n",
    "            samples = []\n",
    "            labels = []\n",
    "            for batch in train_loader:\n",
    "                batch_x, batch_label = batch\n",
    "                x_list = [batch_x]\n",
    "                mean, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
    "                sigma = torch.exp(0.5 * logsigmasq)\n",
    "                eps = Normal(0, 1).sample(mean.shape)\n",
    "                z = mean + eps * sigma\n",
    "                means.append(mean)\n",
    "                samples.append(z)\n",
    "                labels.append(batch_label)\n",
    "\n",
    "        means = torch.vstack(means).cpu()\n",
    "        samples = torch.vstack(samples).cpu()\n",
    "        labels = torch.hstack(labels).cpu()\n",
    "\n",
    "        savepath = os.path.join(dirname, \"latent_samples_epoch_\" + str(epoch) + postfix)\n",
    "        plot_latent_space_with_clusters(samples, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, label_colors, epoch, dpi=dpi)\n",
    "\n",
    "        savepath = os.path.join(dirname, \"latent_means_epoch_\" + str(epoch) + postfix)\n",
    "        plot_latent_space_with_clusters(means, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, label_colors, epoch, dpi=dpi)\n",
    "\n",
    "\n",
    "        # plot samples from generative model\n",
    "        n_gen = n_train\n",
    "        cluster_probs = params['pi_c'].cpu().detach().numpy() #\n",
    "        fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "        for j in range(n_gen):\n",
    "            c = np.random.choice(K, p=cluster_probs)\n",
    "            mu_c = params['mu_c'][c].clone().detach()\n",
    "            sigma_c = torch.exp(0.5 * params['logsigmasq_c'][c]).clone().detach()\n",
    "            z = Normal(0, 1).sample(mu_c.shape) * sigma_c + mu_c\n",
    "            mu_x = decoder.forward(z, decoder_var)[0]\n",
    "            ax.plot(mu_x.cpu().detach().numpy())\n",
    "        fig.savefig(os.path.join(dirname, \"generate_samples_\" + str(epoch) + postfix+ '.png'), dpi=dpi)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    val_elbo = 0\n",
    "    val_mse = 0\n",
    "    val_elbo_term = np.zeros(4)\n",
    "    with torch.no_grad():\n",
    "        for (batch_idx, batch) in enumerate(val_loader):\n",
    "            batch_x, _ = batch\n",
    "            x_list = [batch_x]\n",
    "            mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
    "            sigma = torch.exp(0.5 * logsigmasq)\n",
    "            eps = Normal(0, 1).sample(mu.shape)\n",
    "            z = mu + eps * sigma\n",
    "            with torch.no_grad():\n",
    "                gamma_c, _, _ = em_step(z, mu, logsigmasq, params, em_reg)\n",
    "            elbo, sse, elbo_items = decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c)\n",
    "            val_elbo += elbo.item()\n",
    "            val_mse += sse.item()\n",
    "            val_elbo_term += elbo_items\n",
    "\n",
    "    train_elbo /= len(train_loader.dataset)\n",
    "    train_elbo_term = torch.tensor(train_elbo_term) / len(train_loader.dataset)\n",
    "    val_elbo /= len(val_loader.dataset)\n",
    "    val_elbo_term = torch.tensor(val_elbo_term) / len(val_loader.dataset)\n",
    "    train_mse /= len(train_loader.dataset)\n",
    "    val_mse /= len(val_loader.dataset)\n",
    "\n",
    "    tf = time.time()\n",
    "    toc = time.perf_counter()\n",
    "    print('====> Epoch: {} Train ELBO: {:.4f} Val ELBO: {:.4f}, Epoch Time (s): {:.2f}, Total Time (hrs): {:.4f}'.format(epoch, train_elbo, val_elbo, tf-ti, (toc-tic)/60/60))\n",
    "\n",
    "    train_loss[epoch] = - train_elbo\n",
    "    val_loss[epoch] = - val_elbo\n",
    "    train_elbo_terms[epoch,:] = - train_elbo_term\n",
    "    val_elbo_terms[epoch,:] = - val_elbo_term\n",
    "    pi_history[epoch] = params['pi_c']\n",
    "    train_mse_history[epoch] = train_mse\n",
    "    val_mse_history[epoch] = val_mse\n",
    "\n",
    "    if - val_elbo < min_val_loss:\n",
    "        min_val_loss = - val_elbo\n",
    "        torch.save(params['pi_c'], os.path.join(dirname, 'gmm_params_pi'+ postfix + '.pt'))\n",
    "        torch.save(params['mu_c'], os.path.join(dirname, 'gmm_params_mu'+ postfix + '.pt'))\n",
    "        torch.save(params['logsigmasq_c'], os.path.join(dirname, 'gmm_params_logsigmasq'+ postfix + '.pt'))\n",
    "        torch.save(encoder.state_dict(), os.path.join(dirname, 'encoder'+ postfix + '.pt'))\n",
    "        torch.save(decoder.state_dict(), os.path.join(dirname, 'decoder'+ postfix + '.pt'))\n",
    "\n",
    "    if epoch % plot_interval == 0 or epoch == n_epochs:\n",
    "      # Plot the training and validation loss vs. epoch number\n",
    "      plt.figure(figsize=(4.5, 4))\n",
    "      # const = min(min(train_loss), min(val_loss))\n",
    "      train_loss_adjusted = train_loss\n",
    "      val_loss_adjusted = val_loss\n",
    "      plt.plot(train_loss_adjusted.cpu()[:epoch], label='train')\n",
    "      # print(train_loss_adjusted.cpu()[:epoch])\n",
    "      plt.plot(val_loss_adjusted.cpu()[:epoch], label='val')\n",
    "      plt.yscale('symlog')\n",
    "      plt.xlabel(\"number of epochs\")\n",
    "      plt.ylabel(\"loss\")\n",
    "      plt.title(\"Negative Loss\")\n",
    "      plt.legend()\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(os.path.join(dirname, 'elbo_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "      plt.close()\n",
    "\n",
    "      # Plot each term of the training loss and validation loss\n",
    "      plt.figure(figsize=(4.5, 4))\n",
    "      labels = [\"Reconstruction\", \"GMM Reg\", \"Prob Reg\", \"Encoder Var\"]\n",
    "      for ii in range(4):\n",
    "        train_loss_adjusted = train_elbo_terms[:epoch, ii]\n",
    "        val_loss_adjusted = val_elbo_terms[:epoch, ii]\n",
    "        plt.plot(train_loss_adjusted.cpu()[:epoch], label=f\"{labels[ii]}: Train\")\n",
    "        plt.plot(val_loss_adjusted.cpu()[:epoch], label=f\"{labels[ii]}: Val\", linestyle='--')\n",
    "      plt.xlabel(\"number of epochs\")\n",
    "      plt.yscale('symlog')\n",
    "      plt.ylabel(\"loss\")\n",
    "      plt.title(\"Negative Loss Terms\")\n",
    "      plt.legend()\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(os.path.join(dirname, 'elbo_terms_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "      plt.close()\n",
    "\n",
    "      # Plot the training and validation mse vs. epoch number\n",
    "      plt.figure(figsize=(4.5, 4))\n",
    "      plt.semilogy(train_mse_history.cpu().detach().numpy()[:epoch], label='train')\n",
    "      plt.semilogy(val_mse_history.cpu().detach().numpy()[:epoch], label='val')\n",
    "      plt.xlabel(\"number of epochs\")\n",
    "      plt.legend()\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(os.path.join(dirname, 'reconst_mse_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "      plt.close()\n",
    "\n",
    "      # Plot the history of pi\n",
    "      plt.figure(figsize=(4.5, 4))\n",
    "      for i in range(K):\n",
    "          plt.plot(pi_history[:, i].cpu().detach().numpy()[:epoch], label=r'$\\pi$' + str(i+1))\n",
    "      plt.xlabel(\"number of epochs\")\n",
    "      plt.legend()\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(os.path.join(dirname, 'pi_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "      plt.close()\n",
    "\n",
    "\n",
    "te = time.time()\n",
    "import datetime\n",
    "duration = datetime.timedelta(seconds=te - ts)\n",
    "print(\"Training took \", duration)\n",
    "\n",
    "# Save off pi_history, train_loss, val_loss, train_mse_history, val_mse_history\n",
    "torch.save(pi_history, os.path.join(dirname, 'pi_history'+ postfix + '.pt'))\n",
    "torch.save(train_loss, os.path.join(dirname, 'train_loss'+ postfix + '.pt'))\n",
    "torch.save(val_loss, os.path.join(dirname, 'val_loss'+ postfix + '.pt'))\n",
    "torch.save(train_elbo_terms, os.path.join(dirname, 'train_elbo_terms'+ postfix + '.pt'))\n",
    "torch.save(val_elbo_terms, os.path.join(dirname, 'val_elbo_terms'+ postfix + '.pt'))\n",
    "torch.save(train_mse_history, os.path.join(dirname, 'train_mse_history'+ postfix + '.pt'))\n",
    "torch.save(val_mse_history, os.path.join(dirname, 'val_mse_history'+ postfix + '.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbd092",
   "metadata": {},
   "source": [
    "# Load and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ee665",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30_000\n",
    "params = {}\n",
    "encoder = Encoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd1, hd2, hd3]).to(\"cuda\")\n",
    "decoder = Decoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd3, hd2, hd1]).to(\"cuda\")\n",
    "encoder_list = [encoder]\n",
    "decoder_list = [decoder]\n",
    "\n",
    "device = next(encoder.parameters()).device\n",
    "# Load in training history metrics\n",
    "pi_history = torch.load(os.path.join(dirname, 'pi_history'+ postfix + '.pt'))\n",
    "train_loss = torch.load(os.path.join(dirname, 'train_loss'+ postfix + '.pt'))\n",
    "val_loss = torch.load(os.path.join(dirname, 'val_loss'+ postfix + '.pt'))\n",
    "train_elbo_terms = torch.load(os.path.join(dirname, 'train_elbo_terms'+ postfix + '.pt'))\n",
    "val_elbo_terms = torch.load(os.path.join(dirname, 'val_elbo_terms'+ postfix + '.pt'))\n",
    "train_mse_history = torch.load(os.path.join(dirname, 'train_mse_history'+ postfix + '.pt'))\n",
    "val_mse_history = torch.load(os.path.join(dirname, 'val_mse_history'+ postfix + '.pt'))\n",
    "\n",
    "\n",
    "text_labels = [f'Cluster {i}' for i in range((n_clusters))]\n",
    "label_colors = [f'C{i+1}' for i in range((n_clusters))]\n",
    "data_colors = label_colors\n",
    "\n",
    "# Plot training history\n",
    "# Plot the training and validation loss vs. epoch number\n",
    "plt.figure(figsize=(4, 4))\n",
    "const = min(min(train_loss), min(val_loss))\n",
    "print(const)\n",
    "# const = min(10, const)\n",
    "train_loss_adjusted = train_loss\n",
    "val_loss_adjusted = val_loss\n",
    "plt.plot(train_loss_adjusted.cpu(), label='Training')\n",
    "plt.plot(val_loss_adjusted.cpu(), label='Validation')\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Negative Loss\")\n",
    "# plt.title(\"Negative Loss\")\n",
    "plt.yscale('symlog')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dirname, 'elbo_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "# plt.close()\n",
    "\n",
    "\n",
    " # Plot each term of the training loss and validation loss\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "labels = [\"Reconstruction\", \"GMM Reg\", \"Prob Reg\", \"Encoder Var\"]\n",
    "for ii in range(4):\n",
    "  # print(train_elbo_terms.cpu()[ii, :epoch])\n",
    "  # print(len(train_elbo_terms.cpu()[ii, :epoch]))\n",
    "  train_loss_adjusted = train_elbo_terms[:, ii]\n",
    "  val_loss_adjusted = val_elbo_terms[:, ii]\n",
    "  plt.plot(train_loss_adjusted.cpu()[:], label=f\"{labels[ii]}: Train\")\n",
    "  plt.plot(val_loss_adjusted.cpu()[:], label=f\"{labels[ii]}: Val\", linestyle='--')\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Negative Loss Terms\")\n",
    "plt.yscale('symlog')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dirname, 'elbo_terms_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "plt.close()\n",
    "\n",
    "\n",
    " # Plot the sum of each term of the training loss and validation loss\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "  # print(train_elbo_terms.cpu()[ii, :epoch])\n",
    "  # print(len(train_elbo_terms.cpu()[ii, :epoch]))\n",
    "train_loss_adjusted = train_elbo_terms.sum(axis=1)\n",
    "val_loss_adjusted = val_elbo_terms.sum(axis=1)\n",
    "plt.plot(train_loss_adjusted.cpu()[:], label=f\"Train\")\n",
    "plt.plot(val_loss_adjusted.cpu()[:], label=f\"Val\", linestyle='--')\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Sum of ELBO Terms\")\n",
    "plt.yscale('symlog')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dirname, 'elbo_terms_sum_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "plt.close()\n",
    "\n",
    "# Plot the training and validation mse vs. epoch number\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "plt.semilogy(train_mse_history.cpu().detach().numpy(), label='train')\n",
    "plt.semilogy(val_mse_history.cpu().detach().numpy(), label='val')\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dirname, 'reconst_mse_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "# plt.close()\n",
    "\n",
    "# Plot the history of pi\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(K):\n",
    "    plt.plot(pi_history[:, i].cpu().detach().numpy(), label=text_labels[i]+r' $\\pi$', color=label_colors[i])\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Predicted Cluster Probability\")\n",
    "plt.axhline(y=0.05, color='C1', linestyle='--', label='True Escape Probability')\n",
    "plt.axhline(y=0.95, color='C3', linestyle='--', label='True Capture Probability')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dirname, 'pi_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
    "# plt.close()\n",
    "\n",
    "# Load best model saved\n",
    "params['pi_c'] = torch.load(os.path.join(dirname, 'gmm_params_pi'+ postfix + '.pt'))\n",
    "params['mu_c'] = torch.load(os.path.join(dirname, 'gmm_params_mu'+ postfix + '.pt'))\n",
    "params['logsigmasq_c'] = torch.load(os.path.join(dirname, 'gmm_params_logsigmasq'+ postfix + '.pt'))\n",
    "encoder.load_state_dict(torch.load(os.path.join(dirname, 'encoder'+ postfix + '.pt')))\n",
    "decoder.load_state_dict(torch.load(os.path.join(dirname, 'decoder'+ postfix + '.pt')))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "\n",
    "# run one last EM step and plot training data in latent space\n",
    "for encoder in encoder_list:\n",
    "    encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    means = []\n",
    "    samples = []\n",
    "    labels = []\n",
    "    params['hist_weights'] = torch.zeros((K, 1))\n",
    "    params['hist_mu_c'] = torch.zeros((K, latent_dim))\n",
    "    params['hist_logsigmasq_c'] = torch.zeros((K, latent_dim))\n",
    "    for batch in train_loader:\n",
    "        batch_x, batch_label = batch\n",
    "        x_list = [batch_x]\n",
    "        mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
    "        sigma = torch.exp(0.5 * logsigmasq)\n",
    "        eps = Normal(0, 1).sample(mu.shape)\n",
    "        z = mu + eps * sigma\n",
    "        with torch.no_grad():\n",
    "            gamma_c, mu_c, logsigmasq_c = em_step(z, mu, logsigmasq, params, em_reg, update_by_batch=True)\n",
    "        params['mu_c'] = mu_c\n",
    "        params['logsigmasq_c'] = logsigmasq_c\n",
    "\n",
    "        means.append(mu)\n",
    "        samples.append(z)\n",
    "        labels.append(batch_label)\n",
    "\n",
    "means = torch.vstack(means).cpu()\n",
    "samples = torch.vstack(samples).cpu()\n",
    "labels = torch.hstack(labels).cpu()\n",
    "\n",
    "\n",
    "savepath = os.path.join(dirname, \"BEST_latent_samples\"+postfix)\n",
    "plot_latent_space_with_clusters(samples, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
    "\n",
    "savepath = os.path.join(dirname, \"BEST_latent_means\"+postfix)\n",
    "plot_latent_space_with_clusters(means, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
    "\n",
    "# plot test data in latent space\n",
    "with torch.no_grad():\n",
    "    test_means = []\n",
    "    test_labels = []\n",
    "    for batch in test_loader:\n",
    "        batch_x, batch_label = batch\n",
    "        x_list = [batch_x]\n",
    "        mean, _ = encoder_step(x_list, encoder_list, decoder_list)\n",
    "        test_means.append(mean)\n",
    "        test_labels.append(batch_label)\n",
    "\n",
    "test_means = torch.vstack(test_means).cpu()\n",
    "test_labels = torch.hstack(test_labels).cpu()\n",
    "\n",
    "\n",
    "savepath = os.path.join(dirname, \"BEST_test_latent_samples\"+postfix)\n",
    "plot_latent_space_with_clusters(test_means, test_labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
    "\n",
    "# plot decoding results from cluster means # todo: expand this function for multi-modal data\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "for i in range(K):\n",
    "    # with torch.no_grad:\n",
    "    x_mean = decoder.forward(params['mu_c'][i], decoder_var)[0]\n",
    "    ax.plot(x_mean.cpu().detach().numpy(), label=\"decoded $\\mu$\"+str(i+1))\n",
    "ax.legend()\n",
    "plt.title(\"Decoded Means\")\n",
    "fig.savefig(os.path.join(dirname, \"BEST_decoded_means\"+postfix+'.png'), dpi=dpi)\n",
    "# plt.close()\n",
    "\n",
    "# plot samples from generative model\n",
    "n_gen = n_train\n",
    "cluster_probs = params['pi_c'].cpu().detach().numpy()\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "for j in range(n_gen):\n",
    "    c = np.random.choice(K, p=cluster_probs)\n",
    "    # print(c)\n",
    "    mu_c = params['mu_c'][c].cpu().clone().detach()\n",
    "    sigma_c = torch.exp(0.5 * params['logsigmasq_c'][c]).cpu().clone().detach()\n",
    "    z = Normal(0, 1).sample(mu_c.shape).cpu().clone().detach() * sigma_c + mu_c\n",
    "    # print(z)\n",
    "    mu_x = decoder.forward(z.cuda(), decoder_var)[0].cpu().clone().detach()\n",
    "    sigma_x = torch.exp(0.5 * decoder.forward(z.cuda(), decoder_var)[1])\n",
    "    sample_x = Normal(0, 1).sample(mu_x.shape).cpu().clone().detach() * sigma_x.cpu().clone().detach() + mu_x\n",
    "    ax.plot(sample_x.cpu().detach().numpy())\n",
    "plt.title(\"Generated Samples\")\n",
    "fig.savefig(os.path.join(dirname, \"BEST_generate_samples\"+postfix+'.png'), dpi=dpi)\n",
    "# plt.close()\n",
    "\n",
    "np.savez(dirname + postfix, train_loss=train_loss.cpu().detach().numpy(), val_loss=val_loss.cpu().detach().numpy(),\n",
    "      train_mse=train_mse_history.cpu().detach().numpy(), val_mse=val_mse_history.cpu().detach().numpy(),\n",
    "      pi_history=pi_history.cpu().detach().numpy(),\n",
    "      cluster_probs=params['pi_c'].cpu().detach().numpy(),\n",
    "      cluster_means=params['mu_c'].cpu().detach().numpy(),\n",
    "      cluster_vars=torch.exp(params['logsigmasq_c']).cpu().detach().numpy())\n",
    "\n",
    "\n",
    "print(\"Training data size\", n_train)\n",
    "print(\"Fraction of downward curves:\", (torch.sum(labels == 0) / n_train).item())\n",
    "print(\"Cluster 1 probability:\", cluster_probs.min().item())\n",
    "print(\"Error:\", str(abs(cluster_probs.min().item() - 0.2573)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c51b044f",
      "metadata": {
        "id": "c51b044f"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35fbbdbe",
      "metadata": {
        "id": "35fbbdbe"
      },
      "source": [
        "TO RUN:\n",
        "- Make sure to select Runtime>Change Runtime Type>T4 GPU to use cuda\n",
        "- Install pytorch_lightning\n",
        "- clone in repo to read in common files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "939cf284",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "939cf284",
        "outputId": "3f1ffb0c-8cb2-446c-a895-f02e0b60dca8",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.5.1.post0 torchmetrics-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "97517fef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97517fef",
        "outputId": "ee1d66e6-de12-481e-ea37-fcc56d8323f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED, workers=True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning import LightningDataModule\n",
        "import seaborn as sns\n",
        "import json\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from time import time\n",
        "import datetime\n",
        "import pickle\n",
        "import matplotlib.lines as mlines\n",
        "import sys\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "86396875",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86396875",
        "outputId": "d3a83654-14a9-469b-edaa-3e7cdacf7747",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pipag_training'...\n",
            "remote: Enumerating objects: 294, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 294 (delta 60), reused 58 (delta 30), pack-reused 184 (from 1)\u001b[K\n",
            "Receiving objects: 100% (294/294), 7.71 MiB | 23.63 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://gcalkins64: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@github.com/gcalkins64/pipag_training.git\n",
        "!cd pipag_training && git pull\n",
        "sys.path.append('/content/pipag_training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a86cb23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a86cb23",
        "outputId": "e1631d4a-3687-4140-943b-8512a750c0ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2bb167c4",
      "metadata": {
        "id": "2bb167c4"
      },
      "outputs": [],
      "source": [
        "from gmvae_common import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3180eae7",
      "metadata": {
        "id": "3180eae7"
      },
      "source": [
        "# Check Devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d70b1403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70b1403",
        "outputId": "27bcc341-64be-473c-e2f7-d9aa041e4234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set default tensor type to CUDA tensors\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246109d8",
      "metadata": {
        "id": "246109d8"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9af0a7be",
      "metadata": {
        "id": "9af0a7be"
      },
      "outputs": [],
      "source": [
        "n_train = 1024\n",
        "n_val = 128\n",
        "n_test = 128\n",
        "# For testing small batches of data\n",
        "# n_train = 400\n",
        "# n_val = 50\n",
        "# n_test = 50\n",
        "# THREE LAYERS\n",
        "# hd1 = 48\n",
        "# hd2 = 32\n",
        "# hd3 = 16\n",
        "# hidden_dims = [hd1, hd2, hd3]\n",
        "# TWO LAYERS\n",
        "hd1 = 32\n",
        "hd2 = 16\n",
        "hd3 = None\n",
        "hidden_dims = [hd1, hd2]\n",
        "\n",
        "latent_dims = [5,6,7]\n",
        "n_clustersS = [2,3,4,5,6]\n",
        "skip_combos = [[5,2], [5,3], [5,4], [5,5], [5,6]]\n",
        "skip_combos = []\n",
        "lr = 1e-3\n",
        "n_epochs = 10_000 # 30_000\n",
        "batch_size = 128\n",
        "em_reg = 1e-6\n",
        "decoder_var = 1e-5\n",
        "\n",
        "plot_interval = 500\n",
        "dpi = 300\n",
        "\n",
        "D = 1  #num_modalities\n",
        "downsampleNum = 64\n",
        "\n",
        "loadFlag = False  # True to load in old case, False to run a new case\n",
        "\n",
        "# data = '1_near_escape_fnpag_2000_data_energy_scaled_downsampled_'\n",
        "# inds = '1_near_escape_fnpag_1999_inds_energy_scaled_downsampled_'\n",
        "# tag = 'near_escape'\n",
        "\n",
        "# data = '1_near_crash_fnpag_2000_data_energy_scaled_downsampled_'\n",
        "# inds = '1_near_crash_fnpag_2000_inds_energy_scaled_downsampled_'\n",
        "# tag = 'near_crash'\n",
        "\n",
        "# data = 'UOP_inc_lit_disps_5000_data_energy_scaled_downsampled_'\n",
        "# inds = 'UOP_inc_lit_disps_4999_inds_energy_scaled_downsampled_'\n",
        "# tag = 'near_escape_OLD'\n",
        "\n",
        "# data = 'UOP_near_crash_steeper_5000_data_energy_scaled_downsampled_'\n",
        "# inds = 'UOP_near_crash_steeper_4997_inds_energy_scaled_downsampled_'\n",
        "# tag = 'near_crash_OLD'\n",
        "\n",
        "data = '1_near_escape_new_2000_data_energy_scaled_downsampled_'\n",
        "inds = '1_near_escape_new_2000_inds_energy_scaled_downsampled_'\n",
        "tag = 'near_escape_new'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29af3060",
      "metadata": {
        "id": "29af3060"
      },
      "source": [
        "# LOOP GMVAE TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cb5f8e6f",
      "metadata": {
        "id": "cb5f8e6f"
      },
      "outputs": [],
      "source": [
        "for latent_dim in latent_dims:\n",
        "   for n_clusters in n_clustersS:\n",
        "      if [latent_dim, n_clusters] in skip_combos:\n",
        "        continue\n",
        "      else:\n",
        "          K = n_clusters\n",
        "          Z = latent_dim\n",
        "\n",
        "          from datetime import datetime\n",
        "          timestr = datetime.strftime(datetime.now(), \"%Y%m%d_%H%M%S\")\n",
        "\n",
        "          if loadFlag:  # Load in old data\n",
        "              dirname = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", 'gmvae_em_aerocapture_energy_20250429_155508_5_4')\n",
        "          else:  # generate new data\n",
        "              dirname = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", \"gmvae_\"+tag+\"_\"+timestr+\"_L\"+str(latent_dim)+\"_C\"+str(n_clusters))\n",
        "          os.makedirs(dirname, exist_ok=True)\n",
        "          print(\"Filepath directory: \" + dirname)\n",
        "\n",
        "          if loadFlag:  # load old data\n",
        "              postfix = '_42_1024_128_128_32_16_5_4_0.001000_128_0.001000_0.000010_1000.000000_'\n",
        "          else:  # Generate new suffix\n",
        "              if hd3 is not None:\n",
        "                  postfix = '_{0:d}_{1:d}_{2:d}_{3:d}_{4:d}_{5:d}_{6:d}_{7:d}_{8:d}_{9:f}_{10:d}_{11:f}_{12:f}_{13:f}_'.format(\n",
        "                  SEED, n_train, n_val, n_test, hd1, hd2, hd3, latent_dim, n_clusters, lr, batch_size, em_reg * 1e3, decoder_var, n_epochs)\n",
        "              else:\n",
        "                  postfix = '_{0:d}_{1:d}_{2:d}_{3:d}_{4:d}_{5:d}_{6:d}_{7:d}_{8:f}_{9:d}_{10:f}_{11:f}_{12:f}_'.format(\n",
        "                  SEED, n_train, n_val, n_test, hd1, hd2, latent_dim, n_clusters, lr, batch_size, em_reg * 1e3, decoder_var, n_epochs)\n",
        "          print(\"Filepath postfix: \" + postfix)\n",
        "\n",
        "          data_dir = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", f\"{data}.json\")\n",
        "\n",
        "          with open(os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", f\"{inds}.json\"), 'r') as f:\n",
        "              sample_list_load = json.load(f)\n",
        "          sample_list = [int(sample) for sample in sample_list_load['sample_list']]\n",
        "          print(sample_list)\n",
        "\n",
        "          data_module = AerocaptureDataModuleCUDA(data_dir=data_dir, n_train=n_train, n_val=n_val, n_test=n_test,\n",
        "                                          train_batch=batch_size, val_batch=batch_size, test_batch=batch_size,\n",
        "                                          num_workers=0)\n",
        "\n",
        "          data_module.setup(\"fit\", sample_list = sample_list)\n",
        "\n",
        "          train_loader = data_module.train_dataloader()\n",
        "          val_loader = data_module.val_dataloader()\n",
        "          test_loader = data_module.test_dataloader()\n",
        "          data_dim = len(train_loader.dataset[0][0])\n",
        "          text_labels = ['capture', 'escape', 'impact']\n",
        "          label_colors = ['C2', 'C3', 'C4']\n",
        "\n",
        "          num_train_batches = len(train_loader)\n",
        "\n",
        "          ts_plot = np.linspace(0,450,64)\n",
        "          seabornSettings()\n",
        "          fig, ax = plt.subplots(figsize=(4, 4))\n",
        "          for j in range(n_train):\n",
        "              ax.plot(ts_plot, train_loader.dataset[j][0].cpu(), color=label_colors[train_loader.dataset[j][1].cpu()], alpha=0.75)\n",
        "\n",
        "          eline = mlines.Line2D([], [], color='C2', label='Escape')\n",
        "          cline = mlines.Line2D([], [], color='C3', label='Capture')\n",
        "          iline = mlines.Line2D([], [], color='C4', label='Impact')\n",
        "          plt.legend(handles=[eline, cline, iline])\n",
        "          plt.hlines(0, 0, ts_plot[-1], colors='r', linestyles='dashed')\n",
        "          plt.xlabel(\"Time [s]\")\n",
        "          plt.ylabel(\"Nondimensionalized Energy\")\n",
        "          plt.title(\"Training Data\")\n",
        "          plt.tight_layout()\n",
        "          fig.savefig(os.path.join(dirname, 'train_data'+postfix+'.png'), dpi=dpi)\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "          for j in range(n_val):\n",
        "              ax.plot(ts_plot, val_loader.dataset[j][0].cpu(), color=label_colors[val_loader.dataset[j][1].cpu()])\n",
        "          plt.title(\"Validation Data\")\n",
        "          fig.savefig(os.path.join(dirname, 'val_data'+postfix+'.png'), dpi=dpi)\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "          for j in range(n_test):\n",
        "              ax.plot(ts_plot, test_loader.dataset[j][0].cpu(), color=label_colors[test_loader.dataset[j][1].cpu()])\n",
        "          plt.title(\"Test Data\")\n",
        "          fig.savefig(os.path.join(dirname, 'test_data'+postfix+'.png'), dpi=dpi)\n",
        "          # initialize latent GMM model parameters\n",
        "          params = {}\n",
        "          pi_variables = torch.zeros(K).clone().detach().requires_grad_(True)\n",
        "          params['pi_c'] = torch.ones(K) / K\n",
        "          params['mu_c'] = torch.rand((K, Z)) * 2.0 - 1.0\n",
        "          params['logsigmasq_c'] = torch.zeros((K, Z))\n",
        "\n",
        "\n",
        "          text_labels = [f'Cluster {i}' for i in range((n_clusters))]\n",
        "          label_colors = [f'C{i+1}' for i in range((n_clusters))]\n",
        "\n",
        "          # initialize neural networks\n",
        "          encoder_list = []\n",
        "          decoder_list = []\n",
        "          trainable_parameters = []\n",
        "          trainable_parameters.append(pi_variables)\n",
        "\n",
        "          for _ in range(D):\n",
        "              encoder = Encoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd1, hd2])\n",
        "              decoder = Decoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd2, hd1], decoder_var=decoder_var)\n",
        "              encoder_list.append(encoder)\n",
        "              decoder_list.append(decoder)\n",
        "              trainable_parameters += list(encoder.parameters()) + list(decoder.parameters())\n",
        "\n",
        "          optimizer = optim.Adam(trainable_parameters, lr=lr)\n",
        "\n",
        "          # training\n",
        "\n",
        "          import time\n",
        "          ts = time.time()\n",
        "          tic = time.perf_counter()\n",
        "\n",
        "          train_loss = torch.zeros(n_epochs)\n",
        "          train_elbo_terms = torch.zeros((n_epochs, 4)) # 4 ELBO terms\n",
        "          val_elbo_terms = torch.zeros((n_epochs, 4)) # 4 ELBO terms\n",
        "          val_loss = torch.zeros(n_epochs)\n",
        "          pi_history = torch.zeros((n_epochs, K))\n",
        "          train_mse_history = torch.zeros(n_epochs)\n",
        "          val_mse_history = torch.zeros(n_epochs)\n",
        "          min_val_loss = torch.inf\n",
        "          seabornSettings()\n",
        "\n",
        "          for epoch in range(n_epochs):\n",
        "              ti = time.time()\n",
        "              for encoder in encoder_list:\n",
        "                  encoder.train()\n",
        "              for decoder in decoder_list:\n",
        "                  decoder.train()\n",
        "\n",
        "              train_elbo = 0\n",
        "              train_mse = 0\n",
        "              train_elbo_term = np.zeros(4)\n",
        "              params['hist_weights'] = torch.zeros((K, 1))\n",
        "              params['hist_mu_c'] = torch.zeros((K, latent_dim))\n",
        "              params['hist_logsigmasq_c'] = torch.zeros((K, latent_dim))\n",
        "\n",
        "              for (batch_idx, batch) in enumerate(train_loader):\n",
        "                  batch_x, _ = batch\n",
        "                  x_list = [batch_x]  # assume D=2 and each modality has data_dim\n",
        "                  optimizer.zero_grad()\n",
        "                  pi_c = torch.exp(pi_variables) / torch.sum(torch.exp(pi_variables))\n",
        "                  params['pi_c'] = pi_c\n",
        "\n",
        "                  mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "                  sigma = torch.exp(0.5 * logsigmasq)\n",
        "                  eps = Normal(0, 1).sample(mu.shape)\n",
        "                  z = mu + eps * sigma\n",
        "\n",
        "                  with torch.no_grad():\n",
        "                      gamma_c, mu_c, logsigmasq_c = em_step(z, mu, logsigmasq, params, em_reg, update_by_batch=True)\n",
        "                  params['mu_c'] = mu_c\n",
        "                  params['logsigmasq_c'] = logsigmasq_c\n",
        "\n",
        "                  elbo, sse, elbo_terms = decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c)\n",
        "                  train_elbo += elbo.item()\n",
        "                  train_elbo_term += elbo_terms\n",
        "                  train_mse += sse.item()\n",
        "                  loss = - elbo / batch_x.shape[0]\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "              for encoder in encoder_list:\n",
        "                  encoder.eval()\n",
        "              for decoder in decoder_list:\n",
        "                  decoder.eval()\n",
        "\n",
        "              if epoch % plot_interval == 0 or epoch == n_epochs:\n",
        "                  # Plot the first two dimensions of the latents\n",
        "                  with torch.no_grad():\n",
        "                      means = []\n",
        "                      samples = []\n",
        "                      labels = []\n",
        "                      for batch in train_loader:\n",
        "                          batch_x, batch_label = batch\n",
        "                          x_list = [batch_x]\n",
        "                          mean, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "                          sigma = torch.exp(0.5 * logsigmasq)\n",
        "                          eps = Normal(0, 1).sample(mean.shape)\n",
        "                          z = mean + eps * sigma\n",
        "                          means.append(mean)\n",
        "                          samples.append(z)\n",
        "                          labels.append(batch_label)\n",
        "\n",
        "                  means = torch.vstack(means).cpu()\n",
        "                  samples = torch.vstack(samples).cpu()\n",
        "                  labels = torch.hstack(labels).cpu()\n",
        "\n",
        "                  savepath = os.path.join(dirname, \"latent_samples_epoch_\" + str(epoch) + postfix)\n",
        "                  plot_latent_space_with_clusters(samples, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, label_colors, epoch, dpi=dpi)\n",
        "\n",
        "                  savepath = os.path.join(dirname, \"latent_means_epoch_\" + str(epoch) + postfix)\n",
        "                  plot_latent_space_with_clusters(means, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, label_colors, epoch, dpi=dpi)\n",
        "\n",
        "\n",
        "                  # plot samples from generative model\n",
        "                  n_gen = n_train\n",
        "                  cluster_probs = params['pi_c'].cpu().detach().numpy() #\n",
        "                  fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "                  for j in range(n_gen):\n",
        "                      c = np.random.choice(K, p=cluster_probs)\n",
        "                      mu_c = params['mu_c'][c].clone().detach()\n",
        "                      sigma_c = torch.exp(0.5 * params['logsigmasq_c'][c]).clone().detach()\n",
        "                      z = Normal(0, 1).sample(mu_c.shape) * sigma_c + mu_c\n",
        "                      mu_x = decoder.forward(z)[0]\n",
        "                      ax.plot(mu_x.cpu().detach().numpy())\n",
        "                  fig.savefig(os.path.join(dirname, \"generate_samples_\" + str(epoch) + postfix+ '.png'), dpi=dpi)\n",
        "                  plt.close()\n",
        "\n",
        "\n",
        "              val_elbo = 0\n",
        "              val_mse = 0\n",
        "              val_elbo_term = np.zeros(4)\n",
        "              with torch.no_grad():\n",
        "                  for (batch_idx, batch) in enumerate(val_loader):\n",
        "                      batch_x, _ = batch\n",
        "                      x_list = [batch_x]\n",
        "                      mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "                      sigma = torch.exp(0.5 * logsigmasq)\n",
        "                      eps = Normal(0, 1).sample(mu.shape)\n",
        "                      z = mu + eps * sigma\n",
        "                      with torch.no_grad():\n",
        "                          gamma_c, _, _ = em_step(z, mu, logsigmasq, params, em_reg)\n",
        "                      elbo, sse, elbo_items = decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c)\n",
        "                      val_elbo += elbo.item()\n",
        "                      val_mse += sse.item()\n",
        "                      val_elbo_term += elbo_items\n",
        "\n",
        "              train_elbo /= len(train_loader.dataset)\n",
        "              train_elbo_term = torch.tensor(train_elbo_term) / len(train_loader.dataset)\n",
        "              val_elbo /= len(val_loader.dataset)\n",
        "              val_elbo_term = torch.tensor(val_elbo_term) / len(val_loader.dataset)\n",
        "              train_mse /= len(train_loader.dataset)\n",
        "              val_mse /= len(val_loader.dataset)\n",
        "\n",
        "              tf = time.time()\n",
        "              toc = time.perf_counter()\n",
        "              print('====> Epoch: {} Train ELBO: {:.4f} Val ELBO: {:.4f}, Epoch Time (s): {:.2f}, Total Time (hrs): {:.4f}'.format(epoch, train_elbo, val_elbo, tf-ti, (toc-tic)/60/60))\n",
        "\n",
        "              train_loss[epoch] = - train_elbo\n",
        "              val_loss[epoch] = - val_elbo\n",
        "              train_elbo_terms[epoch,:] = - train_elbo_term\n",
        "              val_elbo_terms[epoch,:] = - val_elbo_term\n",
        "              pi_history[epoch] = params['pi_c']\n",
        "              train_mse_history[epoch] = train_mse\n",
        "              val_mse_history[epoch] = val_mse\n",
        "\n",
        "              if - val_elbo < min_val_loss:\n",
        "                  min_val_loss = - val_elbo\n",
        "                  torch.save(params['pi_c'], os.path.join(dirname, 'gmm_params_pi'+ postfix + '.pt'))\n",
        "                  torch.save(params['mu_c'], os.path.join(dirname, 'gmm_params_mu'+ postfix + '.pt'))\n",
        "                  torch.save(params['logsigmasq_c'], os.path.join(dirname, 'gmm_params_logsigmasq'+ postfix + '.pt'))\n",
        "                  torch.save(encoder.state_dict(), os.path.join(dirname, 'encoder'+ postfix + '.pt'))\n",
        "                  torch.save(decoder.state_dict(), os.path.join(dirname, 'decoder'+ postfix + '.pt'))\n",
        "\n",
        "              if epoch % plot_interval == 0 or epoch == n_epochs:\n",
        "                # Plot the training and validation loss vs. epoch number\n",
        "                plt.figure(figsize=(4.5, 4))\n",
        "                # const = min(min(train_loss), min(val_loss))\n",
        "                train_loss_adjusted = train_loss\n",
        "                val_loss_adjusted = val_loss\n",
        "                plt.plot(train_loss_adjusted.cpu()[:epoch], label='train')\n",
        "                # print(train_loss_adjusted.cpu()[:epoch])\n",
        "                plt.plot(val_loss_adjusted.cpu()[:epoch], label='val')\n",
        "                plt.yscale('symlog')\n",
        "                plt.xlabel(\"number of epochs\")\n",
        "                plt.ylabel(\"loss\")\n",
        "                plt.title(\"Negative Loss\")\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(dirname, 'elbo_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "                plt.close()\n",
        "\n",
        "                # Plot each term of the training loss and validation loss\n",
        "                plt.figure(figsize=(4.5, 4))\n",
        "                labels = [\"Reconstruction\", \"GMM Reg\", \"Prob Reg\", \"Encoder Var\"]\n",
        "                for ii in range(4):\n",
        "                    train_loss_adjusted = train_elbo_terms[:epoch, ii]\n",
        "                    val_loss_adjusted = val_elbo_terms[:epoch, ii]\n",
        "                    plt.plot(train_loss_adjusted.cpu()[:epoch], label=f\"{labels[ii]}: Train\")\n",
        "                    plt.plot(val_loss_adjusted.cpu()[:epoch], label=f\"{labels[ii]}: Val\", linestyle='--')\n",
        "                plt.xlabel(\"number of epochs\")\n",
        "                plt.yscale('symlog')\n",
        "                plt.ylabel(\"loss\")\n",
        "                plt.title(\"Negative Loss Terms\")\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(dirname, 'elbo_terms_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "                plt.close()\n",
        "\n",
        "                # Plot the training and validation mse vs. epoch number\n",
        "                plt.figure(figsize=(4.5, 4))\n",
        "                plt.semilogy(train_mse_history.cpu().detach().numpy()[:epoch], label='train')\n",
        "                plt.semilogy(val_mse_history.cpu().detach().numpy()[:epoch], label='val')\n",
        "                plt.xlabel(\"number of epochs\")\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(dirname, 'reconst_mse_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "                plt.close()\n",
        "\n",
        "                # Plot the history of pi\n",
        "                plt.figure(figsize=(4.5, 4))\n",
        "                for i in range(K):\n",
        "                    plt.plot(pi_history[:, i].cpu().detach().numpy()[:epoch], label=r'$\\pi$' + str(i+1))\n",
        "                plt.xlabel(\"number of epochs\")\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(dirname, 'pi_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "                plt.close()\n",
        "\n",
        "\n",
        "          te = time.time()\n",
        "          import datetime\n",
        "          duration = datetime.timedelta(seconds=te - ts)\n",
        "          print(\"Training took \", duration)\n",
        "\n",
        "          # Save off pi_history, train_loss, val_loss, train_mse_history, val_mse_history\n",
        "          torch.save(pi_history, os.path.join(dirname, 'pi_history'+ postfix + '.pt'))\n",
        "          torch.save(train_loss, os.path.join(dirname, 'train_loss'+ postfix + '.pt'))\n",
        "          torch.save(val_loss, os.path.join(dirname, 'val_loss'+ postfix + '.pt'))\n",
        "          torch.save(train_elbo_terms, os.path.join(dirname, 'train_elbo_terms'+ postfix + '.pt'))\n",
        "          torch.save(val_elbo_terms, os.path.join(dirname, 'val_elbo_terms'+ postfix + '.pt'))\n",
        "          torch.save(train_mse_history, os.path.join(dirname, 'train_mse_history'+ postfix + '.pt'))\n",
        "          torch.save(val_mse_history, os.path.join(dirname, 'val_mse_history'+ postfix + '.pt'))\n",
        "\n",
        "          if loadFlag:\n",
        "            path = dirname\n",
        "            epoch = n_epochs - 1\n",
        "          params = {}\n",
        "          encoder = Encoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd1, hd2]).to(\"cuda\")\n",
        "          decoder = Decoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd2, hd1], decoder_var=decoder_var).to(\"cuda\")\n",
        "\n",
        "          # Use if you need to load in the data\n",
        "          if loadFlag:\n",
        "            suffix = postfix\n",
        "            encoder.load_state_dict(torch.load(os.path.join(path, f'encoder_{suffix}.pt'),map_location=torch.device('cpu')))\n",
        "            decoder.load_state_dict(torch.load(os.path.join(path, f'decoder_{suffix}.pt'),map_location=torch.device('cpu')))\n",
        "            logsigmasq = torch.load(os.path.join(path, f'gmm_params_logsigmasq_{suffix}.pt'),map_location=torch.device('cpu'))\n",
        "            mu = torch.load(os.path.join(path, f'gmm_params_mu_{suffix}.pt'),map_location=torch.device('cpu'))\n",
        "            pi = torch.load(os.path.join(path, f'gmm_params_pi_{suffix}.pt'),map_location=torch.device('cpu'))\n",
        "\n",
        "          encoder_list = [encoder]\n",
        "          decoder_list = [decoder]\n",
        "\n",
        "          device = next(encoder.parameters()).device\n",
        "          # Load in training history metrics\n",
        "          pi_history = torch.load(os.path.join(dirname, 'pi_history'+ postfix + '.pt'))\n",
        "          train_loss = torch.load(os.path.join(dirname, 'train_loss'+ postfix + '.pt'))\n",
        "          val_loss = torch.load(os.path.join(dirname, 'val_loss'+ postfix + '.pt'))\n",
        "          train_elbo_terms = torch.load(os.path.join(dirname, 'train_elbo_terms'+ postfix + '.pt'))\n",
        "          val_elbo_terms = torch.load(os.path.join(dirname, 'val_elbo_terms'+ postfix + '.pt'))\n",
        "          train_mse_history = torch.load(os.path.join(dirname, 'train_mse_history'+ postfix + '.pt'))\n",
        "          val_mse_history = torch.load(os.path.join(dirname, 'val_mse_history'+ postfix + '.pt'))\n",
        "\n",
        "\n",
        "          text_labels = [f'Cluster {i}' for i in range((n_clusters))]\n",
        "          label_colors = [f'C{i+1}' for i in range((n_clusters))]\n",
        "          data_colors = label_colors\n",
        "\n",
        "          # Plot training history\n",
        "          # Plot the training and validation loss vs. epoch number\n",
        "          plt.figure(figsize=(4, 4))\n",
        "          const = min(min(train_loss), min(val_loss))\n",
        "          # const = min(10, const)\n",
        "          train_loss_adjusted = train_loss\n",
        "          val_loss_adjusted = val_loss\n",
        "          plt.plot(train_loss_adjusted.cpu(), label='Training')\n",
        "          plt.plot(val_loss_adjusted.cpu(), label='Validation')\n",
        "          plt.xlabel(\"Number of Epochs\")\n",
        "          plt.ylabel(\"Negative Loss\")\n",
        "          # plt.title(\"Negative Loss\")\n",
        "          plt.yscale('symlog')\n",
        "          plt.legend()\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(os.path.join(dirname, 'elbo_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "          # plt.close()\n",
        "\n",
        "\n",
        "          # Plot each term of the training loss and validation loss\n",
        "          plt.figure(figsize=(4.5, 4))\n",
        "          labels = [\"Reconstruction\", \"GMM Reg\", \"Prob Reg\", \"Encoder Var\"]\n",
        "          for ii in range(4):\n",
        "              # print(train_elbo_terms.cpu()[ii, :epoch])\n",
        "              # print(len(train_elbo_terms.cpu()[ii, :epoch]))\n",
        "              train_loss_adjusted = train_elbo_terms[:, ii]\n",
        "              val_loss_adjusted = val_elbo_terms[:, ii]\n",
        "              plt.plot(train_loss_adjusted.cpu()[:], label=f\"{labels[ii]}: Train\")\n",
        "              plt.plot(val_loss_adjusted.cpu()[:], label=f\"{labels[ii]}: Val\", linestyle='--')\n",
        "          plt.xlabel(\"number of epochs\")\n",
        "          plt.ylabel(\"loss\")\n",
        "          plt.title(\"Negative Loss Terms\")\n",
        "          plt.yscale('symlog')\n",
        "          plt.legend()\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(os.path.join(dirname, 'elbo_terms_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "          # plt.close()\n",
        "\n",
        "\n",
        "          # Plot the sum of each term of the training loss and validation loss\n",
        "          plt.figure(figsize=(4.5, 4))\n",
        "          # print(train_elbo_terms.cpu()[ii, :epoch])\n",
        "          # print(len(train_elbo_terms.cpu()[ii, :epoch]))\n",
        "          train_loss_adjusted = train_elbo_terms.sum(axis=1)\n",
        "          val_loss_adjusted = val_elbo_terms.sum(axis=1)\n",
        "          plt.plot(train_loss_adjusted.cpu()[:], label=f\"Train\")\n",
        "          plt.plot(val_loss_adjusted.cpu()[:], label=f\"Val\", linestyle='--')\n",
        "          plt.xlabel(\"number of epochs\")\n",
        "          plt.ylabel(\"loss\")\n",
        "          plt.title(\"Sum of ELBO Terms\")\n",
        "          plt.yscale('symlog')\n",
        "          plt.legend()\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(os.path.join(dirname, 'elbo_terms_sum_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "          # plt.close()\n",
        "\n",
        "          # Plot the training and validation mse vs. epoch number\n",
        "          plt.figure(figsize=(4.5, 4))\n",
        "          plt.semilogy(train_mse_history.cpu().detach().numpy(), label='train')\n",
        "          plt.semilogy(val_mse_history.cpu().detach().numpy(), label='val')\n",
        "          plt.xlabel(\"number of epochs\")\n",
        "          plt.legend()\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(os.path.join(dirname, 'reconst_mse_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "          # plt.close()\n",
        "\n",
        "          # Plot the history of pi\n",
        "          plt.figure(figsize=(4, 4))\n",
        "          for i in range(K):\n",
        "              plt.plot(pi_history[:, i].cpu().detach().numpy(), label=text_labels[i]+r' $\\pi$', color=label_colors[i])\n",
        "          plt.xlabel(\"Number of Epochs\")\n",
        "          plt.ylabel(\"Predicted Cluster Probability\")\n",
        "          plt.axhline(y=0.05, color='C1', linestyle='--', label='True Escape Probability')\n",
        "          plt.axhline(y=0.95, color='C3', linestyle='--', label='True Capture Probability')\n",
        "          plt.legend()\n",
        "          plt.tight_layout()\n",
        "          plt.savefig(os.path.join(dirname, 'pi_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "          # plt.close()\n",
        "\n",
        "          # Load best model saved\n",
        "          params['pi_c'] = torch.load(os.path.join(dirname, 'gmm_params_pi'+ postfix + '.pt'))\n",
        "          params['mu_c'] = torch.load(os.path.join(dirname, 'gmm_params_mu'+ postfix + '.pt'))\n",
        "          params['logsigmasq_c'] = torch.load(os.path.join(dirname, 'gmm_params_logsigmasq'+ postfix + '.pt'))\n",
        "          encoder.load_state_dict(torch.load(os.path.join(dirname, 'encoder'+ postfix + '.pt')))\n",
        "          decoder.load_state_dict(torch.load(os.path.join(dirname, 'decoder'+ postfix + '.pt')))\n",
        "\n",
        "          encoder.eval()\n",
        "          decoder.eval()\n",
        "\n",
        "          # run one last EM step and plot training data in latent space\n",
        "          for encoder in encoder_list:\n",
        "              encoder.eval()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              means = []\n",
        "              samples = []\n",
        "              labels = []\n",
        "              params['hist_weights'] = torch.zeros((K, 1))\n",
        "              params['hist_mu_c'] = torch.zeros((K, latent_dim))\n",
        "              params['hist_logsigmasq_c'] = torch.zeros((K, latent_dim))\n",
        "              for batch in train_loader:\n",
        "                  batch_x, batch_label = batch\n",
        "                  x_list = [batch_x]\n",
        "                  mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "                  sigma = torch.exp(0.5 * logsigmasq)\n",
        "                  eps = Normal(0, 1).sample(mu.shape)\n",
        "                  z = mu + eps * sigma\n",
        "                  with torch.no_grad():\n",
        "                      gamma_c, mu_c, logsigmasq_c = em_step(z, mu, logsigmasq, params, em_reg, update_by_batch=True)\n",
        "                  params['mu_c'] = mu_c\n",
        "                  params['logsigmasq_c'] = logsigmasq_c\n",
        "\n",
        "                  means.append(mu)\n",
        "                  samples.append(z)\n",
        "                  labels.append(batch_label)\n",
        "\n",
        "          means = torch.vstack(means).cpu()\n",
        "          samples = torch.vstack(samples).cpu()\n",
        "          labels = torch.hstack(labels).cpu()\n",
        "\n",
        "\n",
        "          savepath = os.path.join(dirname, \"BEST_latent_samples\"+postfix)\n",
        "          plot_latent_space_with_clusters(samples, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
        "\n",
        "          savepath = os.path.join(dirname, \"BEST_latent_means\"+postfix)\n",
        "          plot_latent_space_with_clusters(means, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
        "\n",
        "          # plot test data in latent space\n",
        "          with torch.no_grad():\n",
        "              test_means = []\n",
        "              test_labels = []\n",
        "              for batch in test_loader:\n",
        "                  batch_x, batch_label = batch\n",
        "                  x_list = [batch_x]\n",
        "                  mean, _ = encoder_step(x_list, encoder_list, decoder_list)\n",
        "                  test_means.append(mean)\n",
        "                  test_labels.append(batch_label)\n",
        "\n",
        "          test_means = torch.vstack(test_means).cpu()\n",
        "          test_labels = torch.hstack(test_labels).cpu()\n",
        "\n",
        "\n",
        "          savepath = os.path.join(dirname, \"BEST_test_latent_samples\"+postfix)\n",
        "          plot_latent_space_with_clusters(test_means, test_labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
        "\n",
        "          # plot decoding results from cluster means # todo: expand this function for multi-modal data\n",
        "          fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "          for i in range(K):\n",
        "              # with torch.no_grad:\n",
        "              x_mean = decoder.forward(params['mu_c'][i])[0]\n",
        "              ax.plot(x_mean.cpu().detach().numpy(), label=\"decoded $\\mu$\"+str(i+1))\n",
        "          ax.legend()\n",
        "          plt.title(\"Decoded Means\")\n",
        "          fig.savefig(os.path.join(dirname, \"BEST_decoded_means\"+postfix+'.png'), dpi=dpi)\n",
        "          # plt.close()\n",
        "\n",
        "          # plot samples from generative model\n",
        "          n_gen = n_train\n",
        "          cluster_probs = params['pi_c'].cpu().detach().numpy()\n",
        "          fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "          for j in range(n_gen):\n",
        "              c = np.random.choice(K, p=cluster_probs)\n",
        "              # print(c)\n",
        "              mu_c = params['mu_c'][c].cpu().clone().detach()\n",
        "              sigma_c = torch.exp(0.5 * params['logsigmasq_c'][c]).cpu().clone().detach()\n",
        "              z = Normal(0, 1).sample(mu_c.shape).cpu().clone().detach() * sigma_c + mu_c\n",
        "              # print(z)\n",
        "              mu_x = decoder.forward(z.cuda())[0].cpu().clone().detach()\n",
        "              sigma_x = torch.exp(0.5 * decoder.forward(z.cuda())[1])\n",
        "              sample_x = Normal(0, 1).sample(mu_x.shape).cpu().clone().detach() * sigma_x.cpu().clone().detach() + mu_x\n",
        "              ax.plot(sample_x.cpu().detach().numpy())\n",
        "          plt.title(\"Generated Samples\")\n",
        "          fig.savefig(os.path.join(dirname, \"BEST_generate_samples\"+postfix+'.png'), dpi=dpi)\n",
        "          plt.close()\n",
        "\n",
        "          # np.savez(dirname + postfix, train_loss=train_loss.cpu().detach().numpy(), val_loss=val_loss.cpu().detach().numpy(),\n",
        "              # train_mse=train_mse_history.cpu().detach().numpy(), val_mse=val_mse_history.cpu().detach().numpy(),\n",
        "              # pi_history=pi_history.cpu().detach().numpy(),\n",
        "              # cluster_probs=params['pi_c'].cpu().detach().numpy(),\n",
        "              # cluster_means=params['mu_c'].cpu().detach().numpy(),\n",
        "              # cluster_vars=torch.exp(params['logsigmasq_c']).cpu().detach().numpy())\n",
        "\n",
        "\n",
        "          print(\"Training data size\", n_train)\n",
        "          print(\"Fraction of downward curves:\", (torch.sum(labels == 0) / n_train).item())\n",
        "          print(\"Cluster 1 probability:\", cluster_probs.min().item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzyI5tSl5hWc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_cFhN1r6Wzx"
      },
      "source": [
        "TO RUN:\n",
        "- Make sure to select Runtime>Change Runtime Type>T4 GPU to use cuda\n",
        "- Load in data .json\n",
        "- Install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aOVWDm6pDO7",
        "outputId": "b547fb73-e0d4-46b9-c9b1-ef3964e03489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.14)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m362.8/363.4 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EldtogGUqA4c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning import LightningDataModule\n",
        "import seaborn as sns\n",
        "\n",
        "import json\n",
        "import random\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from time import time\n",
        "import datetime\n",
        "import pickle\n",
        "import matplotlib.lines as mlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8M_WeJP5nNX"
      },
      "source": [
        "# Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9xfwWjIqQy5"
      },
      "outputs": [],
      "source": [
        "def seabornSettings():\n",
        "    sns.set_theme('notebook', style='whitegrid', palette='Paired', rc={\"lines.linewidth\": 2.5, \"font.size\": 10, \"axes.titlesize\": 12, \"axes.labelsize\": 12,'xtick.labelsize': 9.0, 'ytick.labelsize': 9.0, \"font.family\": \"serif\"})\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj7b1DR2qX0m"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_latent_space_with_clusters(samples, labels, num_clusters, cluster_means, cluster_logvars, savepath,\n",
        "                                    text_labels, label_colors, data_colors, epoch_num=None, x_min=None, x_max=None, y_min=None, y_max=None, dpi=100):\n",
        "\n",
        "\n",
        "    # savepath = dirname + postfix + 'latent_epoch' + str(epoch)\n",
        "    latent_dim = samples.shape[1]\n",
        "    # print(latent_dim)\n",
        "    # print(samples.shape)\n",
        "    # print(cluster_means.shape)\n",
        "    # print(cluster_logvars.shape)\n",
        "\n",
        "    if latent_dim == 2:\n",
        "        samples_ = samples\n",
        "        cluster_means_ = cluster_means\n",
        "        cluster_stds_ = torch.exp(0.5 * cluster_logvars)\n",
        "        cluster_angles_ = torch.zeros(num_clusters)\n",
        "\n",
        "    elif latent_dim > 2:\n",
        "        pca = PCA(n_components=2)\n",
        "        samples_ = pca.fit_transform(samples)\n",
        "        cluster_means_ = pca.transform(cluster_means)\n",
        "        A = pca.components_  # projection matrix\n",
        "        C = torch.diag_embed(torch.exp(cluster_logvars)) # covariance matrix [num_clusters, latent_dim, latent_dim]\n",
        "        C_proj = np.matmul(np.matmul(A, C), A.T) # [num_clusters, 2, 2]\n",
        "        u, s, vh = np.linalg.svd(C_proj, full_matrices=True)\n",
        "        cluster_stds_ = np.sqrt(s)\n",
        "        cluster_angles_ = np.arctan(u[:, 0, 1] / u[:, 0, 0])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6.5,4))\n",
        "    markers = ['o', '^', \"s\", \"d\"]\n",
        "    assert(len(markers) >= len(text_labels))\n",
        "\n",
        "    for i in range(len(text_labels)):\n",
        "        samples_i = samples_[labels == i]\n",
        "        ax.scatter(samples_i[:, 0], samples_i[:, 1], marker=markers[i], s=50, label=text_labels[i], color=data_colors[i])\n",
        "\n",
        "    for i in range(num_clusters):\n",
        "        ax.plot(cluster_means_[i, 0], cluster_means_[i, 1], 'x', markersize=12, label=text_labels[i]+r' $\\mu$', color=label_colors[i])\n",
        "        ellipse2 = mpatches.Ellipse(xy=cluster_means_[i], width=4.0 * cluster_stds_[i, 0],\n",
        "                                    height=4.0 * cluster_stds_[i, 1],  angle=cluster_angles_[i] * 180 / np.pi,\n",
        "                                    label=text_labels[i]+r' $2\\sigma$', color=label_colors[i], alpha=0.5)\n",
        "        ax.add_patch(ellipse2)\n",
        "\n",
        "    if latent_dim == 2:\n",
        "        ax.set_xlabel('$z_1$')\n",
        "        ax.set_ylabel('$z_2$')\n",
        "    elif latent_dim > 2:\n",
        "        ax.set_xlabel('PC$(z)_1$')\n",
        "        ax.set_ylabel('PC$(z)_2$')\n",
        "\n",
        "    if x_min is not None:\n",
        "        ax.set_xlim([x_min, x_max])\n",
        "    if y_min is not None:\n",
        "        ax.set_ylim([y_min, y_max])\n",
        "    # ax.set_xlim([-90, 80])\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    # ax.legend(loc='best')\n",
        "    if epoch_num is not None:\n",
        "        plt.title(\"Latent Space Epoch {epoch_num}\".format(epoch_num=epoch_num))\n",
        "    else:\n",
        "        plt.title(\"Latent Space\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(savepath + '.png', dpi=dpi)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbJB0VuSqZPF"
      },
      "outputs": [],
      "source": [
        "# Define encoder architecture\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\" Neural network defining q(z | x). \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, latent_dim, hidden_dims=[32, 16, 8]):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=data_dim, out_features=hidden_dims[0]),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_dims[0], out_features=hidden_dims[1]),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_dims[1], out_features=hidden_dims[2]),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_dims[2], out_features=2 * latent_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Returns Normal conditional distribution for q(z | x), with mean and\n",
        "        log-variance output by a neural network.\n",
        "\n",
        "        Args:\n",
        "            x: (N, data_dim) torch.tensor\n",
        "        Returns:\n",
        "            Normal distribution with a batch of (N, latent_dim) means and standard deviations\n",
        "        \"\"\"\n",
        "        # print(\"x device:\", x.get_device())\n",
        "        # print(\"param device:\", list(self.parameters())[0].get_device())\n",
        "\n",
        "        out = self.fc(x)\n",
        "        mu = out[:, 0:latent_dim]\n",
        "        logsigmasq = out[:, latent_dim:]\n",
        "\n",
        "        return mu, logsigmasq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGsQ64eiqgpf"
      },
      "outputs": [],
      "source": [
        "# Define decoder architecture\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\" Neural network defining p(x | z) \"\"\"\n",
        "\n",
        "    def __init__(self, data_dim, latent_dim, hidden_dims=[8, 16, 32]):\n",
        "        super().__init__()\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_dim, out_features=hidden_dims[0]),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_dims[0], out_features=hidden_dims[1]),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_dims[1], out_features=hidden_dims[2]),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=hidden_dims[2], out_features=data_dim),\n",
        "            # nn.ReLU() 11/18 commented out because I realized this was the issue with the velocity dropping to zero\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\" Returns Bernoulli conditional distribution of p(x | z), parametrized\n",
        "        by logits.\n",
        "        Args:\n",
        "            z: (N, latent_dim) torch.tensor\n",
        "        Returns:\n",
        "            Normal distribution with a batch of (N, data_dim)\n",
        "        \"\"\"\n",
        "        out = self.fc(z)\n",
        "        mu = out\n",
        "        logsigmasq = torch.ones_like(mu) * np.log(decoder_var)\n",
        "\n",
        "        return mu, logsigmasq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO9Cli-Vqkzg"
      },
      "outputs": [],
      "source": [
        "def encoder_step(x_list, encoder_list, decoder_list):\n",
        "    \"\"\"\n",
        "    Maps D-modality data to distributions of latent embeddings.\n",
        "    :param x_list: length-D list of (N, data_dim) torch.tensor\n",
        "    :param encoder_list: length-D list of Encoder\n",
        "    :param decoder_list: length-D list of Decoder\n",
        "    :param params: dictionary of non-DNN parameters\n",
        "    :return:\n",
        "        mu: (N, latent_dim) torch.tensor containing the mean of embeddings\n",
        "        sigma: (N, latent_dim) torch.tensor containing the std dev of embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    assert(len(encoder_list) == len(decoder_list))\n",
        "    # assert (len(encoder_list) == len(x_list))\n",
        "\n",
        "    if len(encoder_list) == 1:\n",
        "        mu, logsigmasq = encoder_list[0].forward(x_list[0])\n",
        "\n",
        "    else:\n",
        "        # compute distribution of qz as product of experts\n",
        "        qz_inv_var = 0\n",
        "        qz_mean_inv_var = 0\n",
        "\n",
        "        for d, encoder in enumerate(encoder_list):\n",
        "            mu_, logsigmasq_ = encoder.forward(x_list[d])\n",
        "            qz_inv_var += torch.exp(-logsigmasq_)\n",
        "            qz_mean_inv_var += mu_ * torch.exp(-logsigmasq_)\n",
        "\n",
        "        mu = qz_mean_inv_var / qz_inv_var  # mu = qz_mean\n",
        "        logsigmasq = - torch.log(qz_inv_var)  # sigma = qz_stddev\n",
        "\n",
        "    return mu, logsigmasq\n",
        "\n",
        "def em_step(z, mu, logsigmasq, params, update_by_batch=False):\n",
        "    # compute gamma_c ~ p(c|z) for each x\n",
        "    pi_c = params['pi_c']\n",
        "    mu_c = params['mu_c']  # (K, Z)\n",
        "    logsigmasq_c = params['logsigmasq_c']  # (K, Z)\n",
        "    sigma_c = torch.exp(0.5 * logsigmasq_c)\n",
        "\n",
        "    log_prob_zc = Normal(mu_c, sigma_c).log_prob(z.unsqueeze(dim=1)).sum(dim=2) + torch.log(pi_c)  #[N, K]\n",
        "    log_prob_zc -= log_prob_zc.logsumexp(dim=1, keepdims=True)\n",
        "    gamma_c = torch.exp(log_prob_zc) + em_reg\n",
        "    gamma_c /= gamma_c.sum(dim=1, keepdims=True)\n",
        "\n",
        "    denominator = torch.sum(gamma_c, dim=0).unsqueeze(1)\n",
        "    mu_c = torch.einsum('nc,nz->cz', gamma_c, mu) / denominator\n",
        "    logsigmasq_c = torch.log(torch.einsum('nc,ncz->cz', gamma_c, torch.square(mu.unsqueeze(dim=1) - mu_c) + torch.exp(logsigmasq).unsqueeze(dim=1))) - torch.log(denominator)\n",
        "\n",
        "    if not update_by_batch:\n",
        "        return gamma_c, mu_c, logsigmasq_c\n",
        "\n",
        "    else:\n",
        "        hist_weights = params['hist_weights']\n",
        "        hist_mu_c = params['hist_mu_c']\n",
        "        hist_logsigmasq_c = params['hist_logsigmasq_c']\n",
        "\n",
        "        curr_weights = denominator\n",
        "        new_weights = hist_weights + curr_weights\n",
        "        new_mu_c = (hist_weights * hist_mu_c + curr_weights * mu_c) / new_weights\n",
        "        new_logsigmasq_c = torch.log(hist_weights * torch.exp(hist_logsigmasq_c) +\n",
        "                                      curr_weights * torch.exp(logsigmasq_c)) - torch.log(new_weights)\n",
        "\n",
        "        params['hist_weights'] = new_weights\n",
        "        params['hist_mu_c'] = new_mu_c\n",
        "        params['hist_logsigmasq_c'] = new_logsigmasq_c\n",
        "        return gamma_c, new_mu_c, new_logsigmasq_c\n",
        "\n",
        "\n",
        "\n",
        "def decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c):\n",
        "    \"\"\"\n",
        "    Computes a stochastic estimate of the ELBO.\n",
        "    :param x_list: length-D list of (N, data_dim) torch.tensor\n",
        "    :param z: MC samples of the encoded distributions\n",
        "    :param encoder_list: length-D list of Encoder\n",
        "    :param decoder_list: length-D list of Decoder\n",
        "    :param params: dictionary of non-DNN parameters\n",
        "    :return:\n",
        "        elbo: (,) tensor containing the elbo estimation\n",
        "    \"\"\"\n",
        "    assert(len(encoder_list) == len(decoder_list))\n",
        "\n",
        "    # sigma = torch.exp(0.5 * logsigmasq)\n",
        "    mu_c = params['mu_c']\n",
        "    logsigmasq_c = params['logsigmasq_c']\n",
        "    pi_c = params['pi_c']\n",
        "\n",
        "    sse = 0\n",
        "    elbo = 0\n",
        "    elbo_terms = np.zeros(4)\n",
        "    for d, decoder in enumerate(decoder_list):\n",
        "        mu_, logsigmasq_ = decoder.forward(z)\n",
        "        elbo += Normal(mu_, torch.exp(0.5 * logsigmasq_)).log_prob(x_list[d]).sum()\n",
        "        elbo_terms[0] = Normal(mu_, torch.exp(0.5 * logsigmasq_)).log_prob(x_list[d]).sum()\n",
        "        sse += torch.sum((x_list[d] - mu_) ** 2)\n",
        "    # elbo += - 0.5 * sse\n",
        "    elbo += - 0.5 * torch.sum(gamma_c * (logsigmasq_c + (torch.exp(logsigmasq).unsqueeze(1) + (mu.unsqueeze(1) - mu_c) ** 2) / torch.exp(logsigmasq_c)).sum(dim=2))\n",
        "    elbo_terms[1] = - 0.5 * torch.sum(gamma_c * (logsigmasq_c + (torch.exp(logsigmasq).unsqueeze(1) + (mu.unsqueeze(1) - mu_c) ** 2) / torch.exp(logsigmasq_c)).sum(dim=2))\n",
        "    elbo += torch.sum(gamma_c * (torch.log(pi_c) - torch.log(gamma_c))) + 0.5 * torch.sum(1 + logsigmasq)\n",
        "    elbo_terms[2] = torch.sum(gamma_c * (torch.log(pi_c) - torch.log(gamma_c)))\n",
        "    elbo_terms[3] = 0.5 * torch.sum(1 + logsigmasq)\n",
        "    # print(elbo)\n",
        "    # print(elbo_terms.sum())\n",
        "    # print(elbo_terms)\n",
        "\n",
        "    return elbo, sse, elbo_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhmc2_Z5toh"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC-x5sQ7qrCw"
      },
      "outputs": [],
      "source": [
        "class AerocaptureDataModuleCUDA(LightningDataModule):\n",
        "    def __init__(self, data_dir: str = \"./\", n_train: int = 5000, n_val: int = 100, n_test: int = 100,\n",
        "                 train_batch: int = 1, val_batch: int = 1, test_batch: int = 1, num_workers=8):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.n_train = n_train\n",
        "        self.n_val = n_val\n",
        "        self.n_test = n_test\n",
        "        self.n_samples = n_train + n_val + n_test\n",
        "        self.train_batch = train_batch\n",
        "        self.val_batch = val_batch\n",
        "        self.test_batch = test_batch\n",
        "        self.num_workers = num_workers\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(self.device)\n",
        "        self.downsampleNum = downsampleNum\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        f = open(self.data_dir)\n",
        "        print('Loading File...')\n",
        "        data_dict = json.load(f)\n",
        "        # data_dict = pickle.load(f)\n",
        "        print('...File Loaded')\n",
        "\n",
        "        assert (self.n_samples <= len(data_dict))\n",
        "\n",
        "        data_tr = []\n",
        "        label_tr = []\n",
        "        data_val = []\n",
        "        label_val = []\n",
        "        data_test = []\n",
        "        label_test = []\n",
        "\n",
        "        # Randomize samples\n",
        "        total_samples = len(data_dict)\n",
        "        sample_list = random.sample(range(total_samples), self.n_samples)\n",
        "\n",
        "        # ASSUMES DATA IS ALREADY DOWNSAMPLED AND SCALED\n",
        "        for i in tqdm_notebook(range(self.n_samples)):\n",
        "            j = sample_list[i]\n",
        "            this_data = np.array(data_dict[f'sample{j}']['energy'])[:]\n",
        "            this_label = data_dict[f'sample{j}']['label']\n",
        "\n",
        "            if i >= 0 and i < self.n_train:\n",
        "                data_tr.append(torch.tensor(this_data, dtype=torch.float).to(self.device))\n",
        "                label_tr.append(torch.tensor(this_label, dtype=torch.uint8).to(self.device))\n",
        "            elif i >= self.n_train and i < self.n_train + self.n_val:\n",
        "                data_val.append(torch.tensor(this_data, dtype=torch.float).to(self.device))\n",
        "                label_val.append(torch.tensor(this_label, dtype=torch.uint8).to(self.device))\n",
        "            else:\n",
        "                data_test.append(torch.tensor(this_data, dtype=torch.float).to(self.device))\n",
        "                label_test.append(torch.tensor(this_label, dtype=torch.uint8).to(self.device))\n",
        "\n",
        "        self.train_dataset = tuple(zip(data_tr, label_tr))\n",
        "        self.val_dataset = tuple(zip(data_val, label_val))\n",
        "        self.test_dataset = tuple(zip(data_test, label_test))\n",
        "\n",
        "        self.input_dim = len(self.train_dataset[0][0])\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_stage_dataset = self.train_dataset\n",
        "            self.val_stage_dataset = self.val_dataset\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_stage_dataset = self.test_dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.train_batch, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.val_batch, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.test_batch, shuffle=False, num_workers=self.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdSGDVd54s_"
      },
      "source": [
        "# Check Devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDRsAtYgqcm3"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set default tensor type to CUDA tensors\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)\n",
        "#torch.set_default_tensor_type(torch.FloatTensor)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep1oXshHC4e3"
      },
      "outputs": [],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8CPFPEg59Vi"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vljTKzx9laGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FvLnsqf58uI"
      },
      "outputs": [],
      "source": [
        "r = 0\n",
        "n_train = 1024\n",
        "n_val = 128\n",
        "n_test = 128\n",
        "# For testing small batches of data\n",
        "# n_train = 400\n",
        "# n_val = 50\n",
        "# n_test = 50\n",
        "hd1 = 64\n",
        "hd2 = 32\n",
        "hd3 = 16\n",
        "latent_dim = 4\n",
        "n_clusters = 3\n",
        "lr = 1e-3\n",
        "n_epochs = 30_000 # 30_000\n",
        "batch_size = 64\n",
        "em_reg = 1e-6\n",
        "decoder_var = 1e-5\n",
        "\n",
        "plot_interval = 500\n",
        "dpi = 300\n",
        "\n",
        "K = n_clusters\n",
        "Z = latent_dim\n",
        "D = 1  #num_modalities\n",
        "\n",
        "downsampleNum = 64\n",
        "\n",
        "# Numpy random seed\n",
        "np.random.seed(r)\n",
        "\n",
        "# PyTorch random seed for CPU\n",
        "torch.manual_seed(r)\n",
        "\n",
        "# PyTorch random seed for all GPU operations (if using CUDA)\n",
        "torch.cuda.manual_seed(r)\n",
        "torch.cuda.manual_seed_all(r)\n",
        "\n",
        "# Ensure deterministic behavior for PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2-EP5e26E4Z"
      },
      "source": [
        "# Set Up Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzQFtxrs9-3f"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "timestr = datetime.strftime(datetime.now(), \"%Y%m%d_%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp9k8aBRV8uX"
      },
      "outputs": [],
      "source": [
        "# Load in old data\n",
        "# dirname = os.path.join(\"drive\", \"MyDrive\", \"GMVAE_guided_aerocapture\", 'gmvae_em_aerocapture_energy_20250213_215721')\n",
        "# generate new data path\n",
        "dirname = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", \"gmvae_em_aerocapture_energy_\"+timestr)\n",
        "os.makedirs(dirname, exist_ok=True)\n",
        "print(\"Filepath directory: \" + dirname)\n",
        "\n",
        "postfix = '_{0:d}_{1:d}_{2:d}_{3:d}_{4:d}_{5:d}_{6:d}_{7:d}_{8:d}_{9:f}_{10:d}_{11:f}_{12:f}_{13:d}_'.format(\n",
        "          r, n_train, n_val, n_test, hd1, hd2, hd3, latent_dim, n_clusters, lr, batch_size, em_reg * 1e3, decoder_var, n_epochs)\n",
        "print(\"Filepath postfix: \" + postfix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdlIyjMVsQ1Y"
      },
      "outputs": [],
      "source": [
        "# data = 'jens_vel_data_correct_clusters'\n",
        "# data = 'jens_shorter_data'\n",
        "data = 'neptune_UOP_training_data_2500_scaled_downsampled_log_energy'\n",
        "data_dir = os.path.join(\"drive\", \"MyDrive\", \"JP_gmvae_data\", f\"{data}.json\")\n",
        "# data_dir = f'{data}.pkl'\n",
        "\n",
        "data_module = AerocaptureDataModuleCUDA(data_dir=data_dir, n_train=n_train, n_val=n_val, n_test=n_test,\n",
        "                                  train_batch=batch_size, val_batch=batch_size, test_batch=batch_size,\n",
        "                                  num_workers=0)\n",
        "\n",
        "data_module.setup(\"fit\")\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "test_loader = data_module.test_dataloader()\n",
        "data_dim = len(train_loader.dataset[0][0])\n",
        "text_labels = ['capture', 'escape']\n",
        "label_colors = ['C2', 'C3']\n",
        "\n",
        "num_train_batches = len(train_loader)\n",
        "\n",
        "ts_plot = np.linspace(0,450,64)\n",
        "seabornSettings()\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "for j in range(n_train):\n",
        "    # print(len(train_loader.dataset[j][0].cpu()))\n",
        "    ax.plot(ts_plot, train_loader.dataset[j][0].cpu(), color=label_colors[train_loader.dataset[j][1].cpu()], alpha=0.75)\n",
        "\n",
        "eline = mlines.Line2D([], [], color='C2', label='Escape')\n",
        "cline = mlines.Line2D([], [], color='C3', label='Capture')\n",
        "plt.legend(handles=[eline, cline])\n",
        "plt.hlines(0, 0, ts_plot[-1], colors='r', linestyles='dashed')\n",
        "plt.xlabel(\"Time [s]\")\n",
        "plt.ylabel(\"Nondimensionalized Energy\")\n",
        "plt.title(\"Training Data\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(os.path.join(dirname, 'train_data'+postfix+'.png'), dpi=dpi)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "for j in range(n_val):\n",
        "    ax.scatter(np.arange(downsampleNum), val_loader.dataset[j][0].cpu(), color=label_colors[val_loader.dataset[j][1].cpu()])\n",
        "plt.title(\"Validation Data\")\n",
        "fig.savefig(os.path.join(dirname, 'val_data'+postfix+'.png'), dpi=dpi)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "for j in range(n_test):\n",
        "    ax.scatter(np.arange(downsampleNum), test_loader.dataset[j][0].cpu(), color=label_colors[test_loader.dataset[j][1].cpu()])\n",
        "plt.title(\"Test Data\")\n",
        "fig.savefig(os.path.join(dirname, 'test_data'+postfix+'.png'), dpi=dpi)\n",
        "\n",
        "# Print number of capture / escape in each training set TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJbadl3J6Krs"
      },
      "source": [
        "# Run GMVAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Acgejh2yJNF"
      },
      "outputs": [],
      "source": [
        "# initialize latent GMM model parameters\n",
        "params = {}\n",
        "pi_variables = torch.zeros(K).clone().detach().requires_grad_(True)\n",
        "params['pi_c'] = torch.ones(K) / K\n",
        "params['mu_c'] = torch.rand((K, Z)) * 2.0 - 1.0\n",
        "params['logsigmasq_c'] = torch.zeros((K, Z))\n",
        "\n",
        "# initialize neural networks\n",
        "encoder_list = []\n",
        "decoder_list = []\n",
        "trainable_parameters = []\n",
        "trainable_parameters.append(pi_variables)\n",
        "\n",
        "for _ in range(D):\n",
        "    encoder = Encoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd1, hd2, hd3])\n",
        "    decoder = Decoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd3, hd2, hd1])\n",
        "    encoder_list.append(encoder)\n",
        "    decoder_list.append(decoder)\n",
        "    trainable_parameters += list(encoder.parameters()) + list(decoder.parameters())\n",
        "\n",
        "optimizer = optim.Adam(trainable_parameters, lr=lr)\n",
        "\n",
        "# training\n",
        "\n",
        "import time\n",
        "ts = time.time()\n",
        "tic = time.perf_counter()\n",
        "\n",
        "train_loss = torch.zeros(n_epochs)\n",
        "train_elbo_terms = torch.zeros((n_epochs, 4)) # 4 ELBO terms\n",
        "val_elbo_terms = torch.zeros((n_epochs, 4)) # 4 ELBO terms\n",
        "val_loss = torch.zeros(n_epochs)\n",
        "pi_history = torch.zeros((n_epochs, K))\n",
        "train_mse_history = torch.zeros(n_epochs)\n",
        "val_mse_history = torch.zeros(n_epochs)\n",
        "min_val_loss = torch.inf\n",
        "seabornSettings()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    ti = time.time()\n",
        "    for encoder in encoder_list:\n",
        "        encoder.train()\n",
        "    for decoder in decoder_list:\n",
        "        decoder.train()\n",
        "\n",
        "    train_elbo = 0\n",
        "    train_mse = 0\n",
        "    train_elbo_term = np.zeros(4)\n",
        "    params['hist_weights'] = torch.zeros((K, 1))\n",
        "    params['hist_mu_c'] = torch.zeros((K, latent_dim))\n",
        "    params['hist_logsigmasq_c'] = torch.zeros((K, latent_dim))\n",
        "\n",
        "    for (batch_idx, batch) in enumerate(train_loader):\n",
        "        batch_x, _ = batch\n",
        "        x_list = [batch_x]  # assume D=2 and each modality has data_dim\n",
        "        optimizer.zero_grad()\n",
        "        pi_c = torch.exp(pi_variables) / torch.sum(torch.exp(pi_variables))\n",
        "        params['pi_c'] = pi_c\n",
        "\n",
        "        mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "        sigma = torch.exp(0.5 * logsigmasq)\n",
        "        eps = Normal(0, 1).sample(mu.shape)\n",
        "        z = mu + eps * sigma\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gamma_c, mu_c, logsigmasq_c = em_step(z, mu, logsigmasq, params, update_by_batch=True)\n",
        "        params['mu_c'] = mu_c\n",
        "        params['logsigmasq_c'] = logsigmasq_c\n",
        "\n",
        "        elbo, sse, elbo_terms = decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c)\n",
        "        train_elbo += elbo.item()\n",
        "        train_elbo_term += elbo_terms\n",
        "        train_mse += sse.item()\n",
        "        loss = - elbo / batch_x.shape[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    for encoder in encoder_list:\n",
        "        encoder.eval()\n",
        "    for decoder in decoder_list:\n",
        "        decoder.eval()\n",
        "\n",
        "    if epoch % plot_interval == 0 or epoch == n_epochs:\n",
        "        # Plot the first two dimensions of the latents\n",
        "        with torch.no_grad():\n",
        "            means = []\n",
        "            samples = []\n",
        "            labels = []\n",
        "            for batch in train_loader:\n",
        "                batch_x, batch_label = batch\n",
        "                x_list = [batch_x]\n",
        "                mean, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "                sigma = torch.exp(0.5 * logsigmasq)\n",
        "                eps = Normal(0, 1).sample(mean.shape)\n",
        "                z = mean + eps * sigma\n",
        "                means.append(mean)\n",
        "                samples.append(z)\n",
        "                labels.append(batch_label)\n",
        "\n",
        "        means = torch.vstack(means).cpu()\n",
        "        samples = torch.vstack(samples).cpu()\n",
        "        labels = torch.hstack(labels).cpu()\n",
        "\n",
        "        savepath = os.path.join(dirname, \"latent_samples_epoch_\" + str(epoch) + postfix)\n",
        "        plot_latent_space_with_clusters(samples, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, label_colors, epoch, dpi=dpi)\n",
        "\n",
        "        savepath = os.path.join(dirname, \"latent_means_epoch_\" + str(epoch) + postfix)\n",
        "        plot_latent_space_with_clusters(means, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, label_colors, epoch, dpi=dpi)\n",
        "\n",
        "\n",
        "        # plot samples from generative model\n",
        "        n_gen = n_train\n",
        "        cluster_probs = params['pi_c'].cpu().detach().numpy() #\n",
        "        fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "        for j in range(n_gen):\n",
        "            c = np.random.choice(K, p=cluster_probs)\n",
        "            mu_c = params['mu_c'][c].clone().detach()\n",
        "            sigma_c = torch.exp(0.5 * params['logsigmasq_c'][c]).clone().detach()\n",
        "            z = Normal(0, 1).sample(mu_c.shape) * sigma_c + mu_c\n",
        "            mu_x = decoder.forward(z)[0]\n",
        "            ax.plot(mu_x.cpu().detach().numpy())\n",
        "        fig.savefig(os.path.join(dirname, \"generate_samples_\" + str(epoch) + postfix+ '.png'), dpi=dpi)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    val_elbo = 0\n",
        "    val_mse = 0\n",
        "    val_elbo_term = np.zeros(4)\n",
        "    with torch.no_grad():\n",
        "        for (batch_idx, batch) in enumerate(val_loader):\n",
        "            batch_x, _ = batch\n",
        "            x_list = [batch_x]\n",
        "            mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "            sigma = torch.exp(0.5 * logsigmasq)\n",
        "            eps = Normal(0, 1).sample(mu.shape)\n",
        "            z = mu + eps * sigma\n",
        "            with torch.no_grad():\n",
        "                gamma_c, _, _ = em_step(z, mu, logsigmasq, params)\n",
        "            elbo, sse, elbo_items = decoder_step(x_list, z, encoder_list, decoder_list, params, mu, logsigmasq, gamma_c)\n",
        "            val_elbo += elbo.item()\n",
        "            val_mse += sse.item()\n",
        "            val_elbo_term += elbo_items\n",
        "\n",
        "    train_elbo /= len(train_loader.dataset)\n",
        "    train_elbo_term = torch.tensor(train_elbo_term) / len(train_loader.dataset)\n",
        "    val_elbo /= len(val_loader.dataset)\n",
        "    val_elbo_term = torch.tensor(val_elbo_term) / len(val_loader.dataset)\n",
        "    train_mse /= len(train_loader.dataset)\n",
        "    val_mse /= len(val_loader.dataset)\n",
        "\n",
        "    tf = time.time()\n",
        "    toc = time.perf_counter()\n",
        "    print('====> Epoch: {} Train ELBO: {:.4f} Val ELBO: {:.4f}, Epoch Time (s): {:.2f}, Total Time (hrs): {:.4f}'.format(epoch, train_elbo, val_elbo, tf-ti, (toc-tic)/60/60))\n",
        "\n",
        "    train_loss[epoch] = - train_elbo\n",
        "    val_loss[epoch] = - val_elbo\n",
        "    train_elbo_terms[epoch,:] = - train_elbo_term\n",
        "    val_elbo_terms[epoch,:] = - val_elbo_term\n",
        "    pi_history[epoch] = params['pi_c']\n",
        "    train_mse_history[epoch] = train_mse\n",
        "    val_mse_history[epoch] = val_mse\n",
        "\n",
        "    if - val_elbo < min_val_loss:\n",
        "        min_val_loss = - val_elbo\n",
        "        torch.save(params['pi_c'], os.path.join(dirname, 'gmm_params_pi'+ postfix + '.pt'))\n",
        "        torch.save(params['mu_c'], os.path.join(dirname, 'gmm_params_mu'+ postfix + '.pt'))\n",
        "        torch.save(params['logsigmasq_c'], os.path.join(dirname, 'gmm_params_logsigmasq'+ postfix + '.pt'))\n",
        "        torch.save(encoder.state_dict(), os.path.join(dirname, 'encoder'+ postfix + '.pt'))\n",
        "        torch.save(decoder.state_dict(), os.path.join(dirname, 'decoder'+ postfix + '.pt'))\n",
        "\n",
        "    if epoch % plot_interval == 0 or epoch == n_epochs:\n",
        "      # Plot the training and validation loss vs. epoch number\n",
        "      plt.figure(figsize=(4.5, 4))\n",
        "      # const = min(min(train_loss), min(val_loss))\n",
        "      train_loss_adjusted = train_loss\n",
        "      val_loss_adjusted = val_loss\n",
        "      plt.plot(train_loss_adjusted.cpu()[:epoch], label='train')\n",
        "      # print(train_loss_adjusted.cpu()[:epoch])\n",
        "      plt.plot(val_loss_adjusted.cpu()[:epoch], label='val')\n",
        "      plt.yscale('symlog')\n",
        "      plt.xlabel(\"number of epochs\")\n",
        "      plt.ylabel(\"loss\")\n",
        "      plt.title(\"Negative Loss\")\n",
        "      plt.legend()\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(os.path.join(dirname, 'elbo_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "      plt.close()\n",
        "\n",
        "      # Plot each term of the training loss and validation loss\n",
        "      plt.figure(figsize=(4.5, 4))\n",
        "      labels = [\"Reconstruction\", \"GMM Reg\", \"Prob Reg\", \"Encoder Var\"]\n",
        "      for ii in range(4):\n",
        "        train_loss_adjusted = train_elbo_terms[:epoch, ii]\n",
        "        val_loss_adjusted = val_elbo_terms[:epoch, ii]\n",
        "        plt.plot(train_loss_adjusted.cpu()[:epoch], label=f\"{labels[ii]}: Train\")\n",
        "        plt.plot(val_loss_adjusted.cpu()[:epoch], label=f\"{labels[ii]}: Val\", linestyle='--')\n",
        "      plt.xlabel(\"number of epochs\")\n",
        "      plt.yscale('symlog')\n",
        "      plt.ylabel(\"loss\")\n",
        "      plt.title(\"Negative Loss Terms\")\n",
        "      plt.legend()\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(os.path.join(dirname, 'elbo_terms_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "      plt.close()\n",
        "\n",
        "      # Plot the training and validation mse vs. epoch number\n",
        "      plt.figure(figsize=(4.5, 4))\n",
        "      plt.semilogy(train_mse_history.cpu().detach().numpy()[:epoch], label='train')\n",
        "      plt.semilogy(val_mse_history.cpu().detach().numpy()[:epoch], label='val')\n",
        "      plt.xlabel(\"number of epochs\")\n",
        "      plt.legend()\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(os.path.join(dirname, 'reconst_mse_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "      plt.close()\n",
        "\n",
        "      # Plot the history of pi\n",
        "      plt.figure(figsize=(4.5, 4))\n",
        "      for i in range(K):\n",
        "          plt.plot(pi_history[:, i].cpu().detach().numpy()[:epoch], label=r'$\\pi$' + str(i+1))\n",
        "      plt.xlabel(\"number of epochs\")\n",
        "      plt.legend()\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(os.path.join(dirname, 'pi_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "      plt.close()\n",
        "\n",
        "\n",
        "te = time.time()\n",
        "import datetime\n",
        "duration = datetime.timedelta(seconds=te - ts)\n",
        "print(\"Training took \", duration)\n",
        "\n",
        "# Save off pi_history, train_loss, val_loss, train_mse_history, val_mse_history\n",
        "torch.save(pi_history, os.path.join(dirname, 'pi_history'+ postfix + '.pt'))\n",
        "torch.save(train_loss, os.path.join(dirname, 'train_loss'+ postfix + '.pt'))\n",
        "torch.save(val_loss, os.path.join(dirname, 'val_loss'+ postfix + '.pt'))\n",
        "torch.save(train_elbo_terms, os.path.join(dirname, 'train_elbo_terms'+ postfix + '.pt'))\n",
        "torch.save(val_elbo_terms, os.path.join(dirname, 'val_elbo_terms'+ postfix + '.pt'))\n",
        "torch.save(train_mse_history, os.path.join(dirname, 'train_mse_history'+ postfix + '.pt'))\n",
        "torch.save(val_mse_history, os.path.join(dirname, 'val_mse_history'+ postfix + '.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6JrZewR6Pcz"
      },
      "source": [
        "# Load and Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwSGIp2N5NdA"
      },
      "outputs": [],
      "source": [
        "epoch = 29999\n",
        "epoch = 30_000\n",
        "params = {}\n",
        "encoder = Encoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd1, hd2, hd3]).to(\"cuda\")\n",
        "decoder = Decoder(data_dim=data_dim, latent_dim=latent_dim, hidden_dims=[hd3, hd2, hd1]).to(\"cuda\")\n",
        "encoder_list = [encoder]\n",
        "decoder_list = [decoder]\n",
        "\n",
        "device = next(encoder.parameters()).device\n",
        "# Load in training history metrics\n",
        "pi_history = torch.load(os.path.join(dirname, 'pi_history'+ postfix + '.pt'))\n",
        "train_loss = torch.load(os.path.join(dirname, 'train_loss'+ postfix + '.pt'))\n",
        "val_loss = torch.load(os.path.join(dirname, 'val_loss'+ postfix + '.pt'))\n",
        "train_elbo_terms = torch.load(os.path.join(dirname, 'train_elbo_terms'+ postfix + '.pt'))\n",
        "val_elbo_terms = torch.load(os.path.join(dirname, 'val_elbo_terms'+ postfix + '.pt'))\n",
        "train_mse_history = torch.load(os.path.join(dirname, 'train_mse_history'+ postfix + '.pt'))\n",
        "val_mse_history = torch.load(os.path.join(dirname, 'val_mse_history'+ postfix + '.pt'))\n",
        "\n",
        "\n",
        "text_labels = ['Capture', 'Escape']\n",
        "data_colors = ['C3', 'C1']\n",
        "label_colors = ['C0', 'C2']\n",
        "\n",
        "# Plot training history\n",
        "# Plot the training and validation loss vs. epoch number\n",
        "plt.figure(figsize=(4, 4))\n",
        "const = min(min(train_loss), min(val_loss))\n",
        "print(const)\n",
        "# const = min(10, const)\n",
        "train_loss_adjusted = train_loss\n",
        "val_loss_adjusted = val_loss\n",
        "plt.plot(train_loss_adjusted.cpu(), label='Training')\n",
        "plt.plot(val_loss_adjusted.cpu(), label='Validation')\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Negative Loss\")\n",
        "# plt.title(\"Negative Loss\")\n",
        "plt.yscale('symlog')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(dirname, 'elbo_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "# plt.close()\n",
        "\n",
        "\n",
        " # Plot each term of the training loss and validation loss\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "labels = [\"Reconstruction\", \"GMM Reg\", \"Prob Reg\", \"Encoder Var\"]\n",
        "for ii in range(4):\n",
        "  # print(train_elbo_terms.cpu()[ii, :epoch])\n",
        "  # print(len(train_elbo_terms.cpu()[ii, :epoch]))\n",
        "  train_loss_adjusted = train_elbo_terms[:, ii]\n",
        "  val_loss_adjusted = val_elbo_terms[:, ii]\n",
        "  plt.plot(train_loss_adjusted.cpu()[:], label=f\"{labels[ii]}: Train\")\n",
        "  plt.plot(val_loss_adjusted.cpu()[:], label=f\"{labels[ii]}: Val\", linestyle='--')\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Negative Loss Terms\")\n",
        "plt.yscale('symlog')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(dirname, 'elbo_terms_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "plt.close()\n",
        "\n",
        "\n",
        " # Plot the sum of each term of the training loss and validation loss\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "  # print(train_elbo_terms.cpu()[ii, :epoch])\n",
        "  # print(len(train_elbo_terms.cpu()[ii, :epoch]))\n",
        "train_loss_adjusted = train_elbo_terms.sum(axis=1)\n",
        "val_loss_adjusted = val_elbo_terms.sum(axis=1)\n",
        "plt.plot(train_loss_adjusted.cpu()[:], label=f\"Train\")\n",
        "plt.plot(val_loss_adjusted.cpu()[:], label=f\"Val\", linestyle='--')\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Sum of ELBO Terms\")\n",
        "plt.yscale('symlog')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(dirname, 'elbo_terms_sum_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "plt.close()\n",
        "\n",
        "# Plot the training and validation mse vs. epoch number\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "plt.semilogy(train_mse_history.cpu().detach().numpy(), label='train')\n",
        "plt.semilogy(val_mse_history.cpu().detach().numpy(), label='val')\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(dirname, 'reconst_mse_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "# plt.close()\n",
        "\n",
        "# Plot the history of pi\n",
        "plt.figure(figsize=(4, 4))\n",
        "colors = ['C0', 'C2']\n",
        "for i in range(K):\n",
        "    plt.plot(pi_history[:, i].cpu().detach().numpy(), label=text_labels[i]+r' $\\pi$', color=colors[i])\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Predicted Cluster Probability\")\n",
        "plt.axhline(y=0.649, color='C1', linestyle='--', label='True Escape Probability')\n",
        "plt.axhline(y=0.351, color='C3', linestyle='--', label='True Capture Probability')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(dirname, 'pi_'+ str(epoch)+postfix+'.png'), dpi=dpi)\n",
        "# plt.close()\n",
        "\n",
        "# Load best model saved\n",
        "params['pi_c'] = torch.load(os.path.join(dirname, 'gmm_params_pi'+ postfix + '.pt'))\n",
        "params['mu_c'] = torch.load(os.path.join(dirname, 'gmm_params_mu'+ postfix + '.pt'))\n",
        "params['logsigmasq_c'] = torch.load(os.path.join(dirname, 'gmm_params_logsigmasq'+ postfix + '.pt'))\n",
        "encoder.load_state_dict(torch.load(os.path.join(dirname, 'encoder'+ postfix + '.pt')))\n",
        "decoder.load_state_dict(torch.load(os.path.join(dirname, 'decoder'+ postfix + '.pt')))\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "\n",
        "# run one last EM step and plot training data in latent space\n",
        "for encoder in encoder_list:\n",
        "    encoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    means = []\n",
        "    samples = []\n",
        "    labels = []\n",
        "    params['hist_weights'] = torch.zeros((K, 1))\n",
        "    params['hist_mu_c'] = torch.zeros((K, latent_dim))\n",
        "    params['hist_logsigmasq_c'] = torch.zeros((K, latent_dim))\n",
        "    for batch in train_loader:\n",
        "        batch_x, batch_label = batch\n",
        "        x_list = [batch_x]\n",
        "        mu, logsigmasq = encoder_step(x_list, encoder_list, decoder_list)\n",
        "        sigma = torch.exp(0.5 * logsigmasq)\n",
        "        eps = Normal(0, 1).sample(mu.shape)\n",
        "        z = mu + eps * sigma\n",
        "        with torch.no_grad():\n",
        "            gamma_c, mu_c, logsigmasq_c = em_step(z, mu, logsigmasq, params, update_by_batch=True)\n",
        "        params['mu_c'] = mu_c\n",
        "        params['logsigmasq_c'] = logsigmasq_c\n",
        "\n",
        "        means.append(mu)\n",
        "        samples.append(z)\n",
        "        labels.append(batch_label)\n",
        "\n",
        "means = torch.vstack(means).cpu()\n",
        "samples = torch.vstack(samples).cpu()\n",
        "labels = torch.hstack(labels).cpu()\n",
        "\n",
        "\n",
        "savepath = os.path.join(dirname, \"BEST_latent_samples\"+postfix)\n",
        "plot_latent_space_with_clusters(samples, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
        "\n",
        "savepath = os.path.join(dirname, \"BEST_latent_means\"+postfix)\n",
        "plot_latent_space_with_clusters(means, labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
        "\n",
        "# plot test data in latent space\n",
        "with torch.no_grad():\n",
        "    test_means = []\n",
        "    test_labels = []\n",
        "    for batch in test_loader:\n",
        "        batch_x, batch_label = batch\n",
        "        x_list = [batch_x]\n",
        "        mean, _ = encoder_step(x_list, encoder_list, decoder_list)\n",
        "        test_means.append(mean)\n",
        "        test_labels.append(batch_label)\n",
        "\n",
        "test_means = torch.vstack(test_means).cpu()\n",
        "test_labels = torch.hstack(test_labels).cpu()\n",
        "\n",
        "\n",
        "savepath = os.path.join(dirname, \"BEST_test_latent_samples\"+postfix)\n",
        "plot_latent_space_with_clusters(test_means, test_labels, K, mu_c.cpu(), logsigmasq_c.cpu(), savepath, text_labels, label_colors, data_colors, dpi=dpi)\n",
        "\n",
        "# plot decoding results from cluster means # todo: expand this function for multi-modal data\n",
        "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "for i in range(K):\n",
        "    # with torch.no_grad:\n",
        "    x_mean = decoder.forward(params['mu_c'][i])[0]\n",
        "    ax.plot(x_mean.cpu().detach().numpy(), label=\"decoded $\\mu$\"+str(i+1))\n",
        "ax.legend()\n",
        "plt.title(\"Decoded Means\")\n",
        "fig.savefig(os.path.join(dirname, \"BEST_decoded_means\"+postfix+'.png'), dpi=dpi)\n",
        "# plt.close()\n",
        "\n",
        "# plot samples from generative model\n",
        "n_gen = n_train\n",
        "cluster_probs = params['pi_c'].cpu().detach().numpy()\n",
        "fig, ax = plt.subplots(figsize=(4.5, 4))\n",
        "for j in range(n_gen):\n",
        "    c = np.random.choice(K, p=cluster_probs)\n",
        "    # print(c)\n",
        "    mu_c = params['mu_c'][c].cpu().clone().detach()\n",
        "    sigma_c = torch.exp(0.5 * params['logsigmasq_c'][c]).cpu().clone().detach()\n",
        "    z = Normal(0, 1).sample(mu_c.shape).cpu().clone().detach() * sigma_c + mu_c\n",
        "    # print(z)\n",
        "    mu_x = decoder.forward(z.cuda())[0].cpu().clone().detach()\n",
        "    sigma_x = torch.exp(0.5 * decoder.forward(z.cuda())[1])\n",
        "    sample_x = Normal(0, 1).sample(mu_x.shape).cpu().clone().detach() * sigma_x.cpu().clone().detach() + mu_x\n",
        "    ax.plot(sample_x.cpu().detach().numpy())\n",
        "plt.title(\"Generated Samples\")\n",
        "fig.savefig(os.path.join(dirname, \"BEST_generate_samples\"+postfix+'.png'), dpi=dpi)\n",
        "# plt.close()\n",
        "\n",
        "np.savez(dirname + postfix, train_loss=train_loss.cpu().detach().numpy(), val_loss=val_loss.cpu().detach().numpy(),\n",
        "      train_mse=train_mse_history.cpu().detach().numpy(), val_mse=val_mse_history.cpu().detach().numpy(),\n",
        "      pi_history=pi_history.cpu().detach().numpy(),\n",
        "      cluster_probs=params['pi_c'].cpu().detach().numpy(),\n",
        "      cluster_means=params['mu_c'].cpu().detach().numpy(),\n",
        "      cluster_vars=torch.exp(params['logsigmasq_c']).cpu().detach().numpy())\n",
        "\n",
        "\n",
        "print(\"Training data size\", n_train)\n",
        "print(\"Fraction of downward curves:\", (torch.sum(labels == 0) / n_train).item())\n",
        "print(\"Cluster 1 probability:\", cluster_probs.min().item())\n",
        "print(\"Error:\", str(abs(cluster_probs.min().item() - 0.2573)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1zYEE5JZo1feEu8dbGyZggx_9Pl5k4-qM",
      "authorship_tag": "ABX9TyN+rvWu2ZCk2ZRcjVWK2w8z"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
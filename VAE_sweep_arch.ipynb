{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1547 samples with label 0.\n",
      "total loss function calls:  60429.6875\n",
      "Number of cases:  72\n",
      "LD: 6, HD: [64, 48, 32, 16], Beta: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 384/5000 [00:09<01:52, 41.10 Epoch/s, loss=0.0217]"
     ]
    }
   ],
   "source": [
    "import torch # we need pytorch installed\n",
    "from models import VAE # import the VAE model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import json\n",
    "from scipy.io import savemat\n",
    "\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n",
    "\n",
    "# Make determinsitic\n",
    "SEED = 42  # or any fixed number\n",
    "\n",
    "# Python, NumPy, PyTorch seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# For deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Optional: ensure reproducibility across runs\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # Needed for CUDA <11.2\n",
    "\n",
    "\n",
    "#%% SETTINGS\n",
    "DataType = 'cn'\n",
    "LatentDims = [6,7,8]\n",
    "HiddenDimsList = [np.array([64,48,32,16]),\n",
    "                  np.array([64,32,16]),\n",
    "                  np.array([32,16,8]),\n",
    "                  np.array([48,32,16]),]\n",
    "Betas = [0.0013, 0.0014, 0.0015, 0.0016, 0.0017, 0.0018]\n",
    "BatchSize = 128\n",
    "Epochs = 5000\n",
    "tag = 'UOP_uniform_pGRAM' # UOP_near_crash_steeper  UOP_near_crash  UOP_inc_lit_disps\n",
    "\n",
    "# load training data\n",
    "json_file = f'/Users/gracecalkins/Local_Documents/local_code/pipag_training/data/UOP_uniform_pGRAM_2000_data_energy_scaled_downsampled_.json'\n",
    "# json_file = f'/Users/gracecalkins/Local_Documents/local_code/pipag_training/data/UOP_near_crash_steeper_near_escape_COMBINED_5000_data_energy_scaled_downsampled_.json'\n",
    "\n",
    "date = datetime.datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "\n",
    "# Create parent 'figs' folder if it doesn't exist\n",
    "figPath = './VAE_arch_eval_round2'\n",
    "os.makedirs(figPath, exist_ok=True)\n",
    "\n",
    "\n",
    "# set device\n",
    "Dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# dataset is a dict: sample0, sample1, etc.\n",
    "# Collect energy arrays where label == 0 (capture scenarios)\n",
    "filtered_data = [entry['energy'] for entry in dataset.values() if entry['label'] == 0]\n",
    "\n",
    "# Convert to numpy array\n",
    "data = np.array(filtered_data)\n",
    "\n",
    "n_data, k_data = data.shape # define number of samples and features\n",
    "\n",
    "x_trn = torch.from_numpy(data).type(torch.FloatTensor).to(Dev)\n",
    "x_trn_cpu = x_trn.cpu()\n",
    "\n",
    "print('Loaded {} samples with label 0.'.format(len(filtered_data)))\n",
    "print('total loss function calls: ', len(data)/BatchSize*Epochs)\n",
    "\n",
    "\n",
    "# Loop over latent dimensions, hidden dimensions, and betas\n",
    "numCases = len(LatentDims) * len(HiddenDimsList) * len(Betas)\n",
    "print('Number of cases: ', numCases)\n",
    "\n",
    "sum_time = 0\n",
    "complete = 0\n",
    "\n",
    "# Loop over all combinations of latent dimensions, hidden dimensions, and betas\n",
    "for LatentDim in LatentDims:\n",
    "    for HiddenDimsArr in HiddenDimsList:\n",
    "        for Beta in Betas:\n",
    "            tic = time.time()\n",
    "            HiddenDims = HiddenDimsArr.tolist()\n",
    "            print(f\"LD: {LatentDim}, HD: {HiddenDims}, Beta: {Beta}\")\n",
    "\n",
    "            #%% create directory for saving figures\n",
    "            # Create a directory for the current configuration\n",
    "            suffix = f'{tag}_{DataType}_dim{LatentDim}_'\n",
    "            for i, d in enumerate(HiddenDims):\n",
    "                suffix += str(HiddenDims[i])\n",
    "                if i < len(HiddenDims) - 1:\n",
    "                    suffix += 'x'\n",
    "            suffix += f'_beta{Beta}_batch{BatchSize}_epochs{Epochs}_{date}'\n",
    "\n",
    "\n",
    "            #%% train\n",
    "            vae_model = VAE(k_data,\n",
    "                            latent_dim = LatentDim, # define the latent dimension of the auto-encoder\n",
    "                            hidden_dims = HiddenDims, # define the strucute of the encoder and decoder\n",
    "                            beta = Beta,\n",
    "                            dev = Dev\n",
    "                        )\n",
    "            vae_model.to(Dev)\n",
    "            epoch_loss, rec_loss, kld_loss = vae_model.train(x_trn,\n",
    "                                                        batch_size = BatchSize,\n",
    "                                                        epochs = Epochs\n",
    "                                                        ) # train the vae model using synthetic data\n",
    "            \n",
    "\n",
    "            #%% plots\n",
    "            # total loss\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.grid()\n",
    "            ax.plot(epoch_loss, '.')\n",
    "            ax.set_ylim(-1, 30)\n",
    "            plt.savefig(os.path.join(figPath, f'loss_{suffix}.png'))\n",
    "\n",
    "            # loss sources\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.grid()\n",
    "            ax.plot(rec_loss, '.', label = 'reconstruction')\n",
    "            ax.plot(kld_loss, '.', label = 'kld')\n",
    "            ax.legend()\n",
    "            plt.savefig(os.path.join(figPath, f'lossTerms_{suffix}.png'))\n",
    "\n",
    "            # compare variance\n",
    "            big_samp = vae_model.sample(num_samples = n_data).cpu()\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.grid()\n",
    "            ax.plot(stats.describe(x_trn_cpu, axis = 0)[3], label = 'Training Variance')\n",
    "            ax.plot(stats.describe(big_samp.detach(), axis = 0)[3], label = 'VAE Variance')\n",
    "            ax.legend()\n",
    "            plt.savefig(os.path.join(figPath, f'var_{suffix}.png'))\n",
    "            plt.close('all')\n",
    "\n",
    "            toc = time.time()\n",
    "            run_time = toc - tic\n",
    "            avg_time = (sum_time+run_time)/(complete+1)\n",
    "            time_left = (numCases-complete)*avg_time\n",
    "            sum_time += run_time\n",
    "            complete += 1\n",
    "            print(f\"Estimated time left: {time_left/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
